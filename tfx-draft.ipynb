{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 09:28:15.408734: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-31 09:28:15.412176: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-31 09:28:15.412188: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FEATURE_KEYS = [\n",
    "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
    "]\n",
    "_LABEL_KEY = 'species'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "           for feature in _FEATURE_KEYS\n",
    "       },\n",
    "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainer_module_file = 'penguin_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguin_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "_FEATURE_KEYS = [\n",
    "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
    "]\n",
    "_LABEL_KEY = 'species'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "# Since we're not generating or creating a schema, we will instead create\n",
    "# a feature spec.  Since there are a fairly small number of features this is\n",
    "# manageable for this dataset.\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "           for feature in _FEATURE_KEYS\n",
    "       },\n",
    "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    schema: schema of the input data.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(2):\n",
    "    d = keras.layers.Dense(8, activation='relu')(d)\n",
    "  outputs = keras.layers.Dense(3)(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(1e-2),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
    "  # version provided by pipeline author. A schema can also derived from TFT\n",
    "  # graph if a Transform component is used. In the case when either is missing,\n",
    "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
    "  # feature_spec, but the schema returned would be very primitive.\n",
    "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "  # directory.\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n",
    "  # Brings data into the pipeline.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  # Uses user-provided Python function that trains a model.\n",
    "  trainer = tfx.components.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
    "\n",
    "  # Pushes the model to a filesystem destination.\n",
    "  pusher = tfx.components.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      push_destination=tfx.proto.PushDestination(\n",
    "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "              base_directory=serving_model_dir)))\n",
    "\n",
    "  # Following three components will be included in the pipeline.\n",
    "  components = [\n",
    "      example_gen,\n",
    "      trainer,\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying penguin_trainer.py -> build/lib\n",
      "installing to /tmp/tmpwxxmq8u8\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/penguin_trainer.py -> /tmp/tmpwxxmq8u8\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmpwxxmq8u8/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3.9.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpwxxmq8u8/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.dist-info/WHEEL\n",
      "creating '/tmp/tmpkeyk41y0/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl' and adding '/tmp/tmpwxxmq8u8' to it\n",
      "adding 'penguin_trainer.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.dist-info/RECORD'\n",
      "removing /tmp/tmpwxxmq8u8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesusarguelles/miniconda3/envs/tfx19/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:absl:Execution 3 failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Split pattern /tmp/tfx-data3e4ecj0p/* does not match any files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m SERVING_MODEL_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mserving_model\u001b[39m\u001b[39m'\u001b[39m, PIPELINE_NAME)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m METADATA_PATH \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m, PIPELINE_NAME, \u001b[39m'\u001b[39m\u001b[39mmetadata.db\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tfx\u001b[39m.\u001b[39;49morchestration\u001b[39m.\u001b[39;49mLocalDagRunner()\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   _create_pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m       pipeline_name\u001b[39m=\u001b[39;49mPIPELINE_NAME,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m       pipeline_root\u001b[39m=\u001b[39;49mPIPELINE_ROOT,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m       data_root\u001b[39m=\u001b[39;49mDATA_ROOT,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m       module_file\u001b[39m=\u001b[39;49m_trainer_module_file,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m       serving_model_dir\u001b[39m=\u001b[39;49mSERVING_MODEL_DIR,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jesusarguelles/code/vertex-gpu/tfx-draft.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m       metadata_path\u001b[39m=\u001b[39;49mMETADATA_PATH))\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/tfx/orchestration/portable/tfx_runner.py:124\u001b[0m, in \u001b[0;36mIrBasedRunner.run\u001b[0;34m(self, pipeline, run_options, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m   run_options_pb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_with_ir(pipeline_pb, run_options\u001b[39m=\u001b[39;49mrun_options_pb, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/tfx/orchestration/local/local_dag_runner.py:109\u001b[0m, in \u001b[0;36mLocalDagRunner.run_with_ir\u001b[0;34m(self, pipeline, run_options)\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[39mwith\u001b[39;00m metadata\u001b[39m.\u001b[39mMetadata(connection_config) \u001b[39mas\u001b[39;00m mlmd_handle:\n\u001b[1;32m    108\u001b[0m     partial_run_utils\u001b[39m.\u001b[39msnapshot(mlmd_handle, pipeline)\n\u001b[0;32m--> 109\u001b[0m component_launcher\u001b[39m.\u001b[39;49mlaunch()\n\u001b[1;32m    110\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mComponent \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is finished.\u001b[39m\u001b[39m'\u001b[39m, node_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/tfx/orchestration/portable/launcher.py:571\u001b[0m, in \u001b[0;36mLauncher.launch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor_operator\u001b[39m.\u001b[39mwith_execution_watcher(\n\u001b[1;32m    569\u001b[0m         executor_watcher\u001b[39m.\u001b[39maddress)\n\u001b[1;32m    570\u001b[0m     executor_watcher\u001b[39m.\u001b[39mstart()\n\u001b[0;32m--> 571\u001b[0m   executor_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_executor(execution_info)\n\u001b[1;32m    572\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    573\u001b[0m   execution_output \u001b[39m=\u001b[39m (\n\u001b[1;32m    574\u001b[0m       e\u001b[39m.\u001b[39mexecutor_output \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, _ExecutionFailedError) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/tfx/orchestration/portable/launcher.py:446\u001b[0m, in \u001b[0;36mLauncher._run_executor\u001b[0;34m(self, execution_info)\u001b[0m\n\u001b[1;32m    444\u001b[0m outputs_utils\u001b[39m.\u001b[39mmake_output_dirs(execution_info\u001b[39m.\u001b[39moutput_dict)\n\u001b[1;32m    445\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 446\u001b[0m   executor_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executor_operator\u001b[39m.\u001b[39;49mrun_executor(execution_info)\n\u001b[1;32m    447\u001b[0m   code \u001b[39m=\u001b[39m executor_output\u001b[39m.\u001b[39mexecution_result\u001b[39m.\u001b[39mcode\n\u001b[1;32m    448\u001b[0m   \u001b[39mif\u001b[39;00m code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/tfx/orchestration/portable/beam_executor_operator.py:98\u001b[0m, in \u001b[0;36mBeamExecutorOperator.run_executor\u001b[0;34m(self, execution_info, make_beam_pipeline_fn)\u001b[0m\n\u001b[1;32m     86\u001b[0m context \u001b[39m=\u001b[39m base_beam_executor\u001b[39m.\u001b[39mBaseBeamExecutor\u001b[39m.\u001b[39mContext(\n\u001b[1;32m     87\u001b[0m     beam_pipeline_args\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_pipeline_args,\n\u001b[1;32m     88\u001b[0m     extra_flags\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextra_flags,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     pipeline_run_id\u001b[39m=\u001b[39mexecution_info\u001b[39m.\u001b[39mpipeline_run_id,\n\u001b[1;32m     96\u001b[0m     make_beam_pipeline_fn\u001b[39m=\u001b[39mmake_beam_pipeline_fn)\n\u001b[1;32m     97\u001b[0m executor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor_cls(context\u001b[39m=\u001b[39mcontext)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m python_executor_operator\u001b[39m.\u001b[39;49mrun_with_executor(execution_info, executor)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/tfx/orchestration/portable/python_executor_operator.py:58\u001b[0m, in \u001b[0;36mrun_with_executor\u001b[0;34m(execution_info, executor)\u001b[0m\n\u001b[1;32m     55\u001b[0m       artifact\u001b[39m.\u001b[39mread()\n\u001b[1;32m     57\u001b[0m output_dict \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(execution_info\u001b[39m.\u001b[39moutput_dict)\n\u001b[0;32m---> 58\u001b[0m result \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39;49mDo(execution_info\u001b[39m.\u001b[39;49minput_dict, output_dict,\n\u001b[1;32m     59\u001b[0m                      execution_info\u001b[39m.\u001b[39;49mexec_properties)\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[1;32m     61\u001b[0m   \u001b[39m# If result is not returned from the Do function, then try to\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[39m# read from the executor_output_uri.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m fileio\u001b[39m.\u001b[39mexists(execution_info\u001b[39m.\u001b[39mexecution_output_uri):\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/tfx/components/example_gen/base_example_gen_executor.py:274\u001b[0m, in \u001b[0;36mBaseExampleGenExecutor.Do\u001b[0;34m(self, input_dict, output_dict, exec_properties)\u001b[0m\n\u001b[1;32m    272\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mGenerating examples.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    273\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_beam_pipeline() \u001b[39mas\u001b[39;00m pipeline:\n\u001b[0;32m--> 274\u001b[0m   example_splits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mGenerateExamplesByBeam(pipeline, exec_properties)\n\u001b[1;32m    276\u001b[0m   \u001b[39m# pylint: disable=expression-not-assigned, no-value-for-parameter\u001b[39;00m\n\u001b[1;32m    277\u001b[0m   \u001b[39mfor\u001b[39;00m split_name, example_split \u001b[39min\u001b[39;00m example_splits\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/tfx/components/example_gen/base_example_gen_executor.py:196\u001b[0m, in \u001b[0;36mBaseExampleGenExecutor.GenerateExamplesByBeam\u001b[0;34m(self, pipeline, exec_properties)\u001b[0m\n\u001b[1;32m    193\u001b[0m     total_buckets \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m split\u001b[39m.\u001b[39mhash_buckets\n\u001b[1;32m    194\u001b[0m     buckets\u001b[39m.\u001b[39mappend(total_buckets)\n\u001b[1;32m    195\u001b[0m   example_splits \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 196\u001b[0m       pipeline\n\u001b[1;32m    197\u001b[0m       \u001b[39m|\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mInputToRecord\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m>>\u001b[39;49m\n\u001b[1;32m    198\u001b[0m       \u001b[39m# pylint: disable=no-value-for-parameter\u001b[39;49;00m\n\u001b[1;32m    199\u001b[0m       input_to_record(exec_properties, input_config\u001b[39m.\u001b[39;49msplits[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mpattern)\n\u001b[1;32m    200\u001b[0m       \u001b[39m|\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSplitData\u001b[39m\u001b[39m'\u001b[39m \u001b[39m>>\u001b[39m beam\u001b[39m.\u001b[39mPartition(_PartitionFn, \u001b[39mlen\u001b[39m(buckets), buckets,\n\u001b[1;32m    201\u001b[0m                                       output_config\u001b[39m.\u001b[39msplit_config))\n\u001b[1;32m    202\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m   \u001b[39m# Use input splits.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m   \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m input_config\u001b[39m.\u001b[39msplits:\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/apache_beam/transforms/ptransform.py:1095\u001b[0m, in \u001b[0;36m_NamedPTransform.__ror__\u001b[0;34m(self, pvalueish, _unused)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__ror__\u001b[39m(\u001b[39mself\u001b[39m, pvalueish, _unused\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1095\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform\u001b[39m.\u001b[39;49m\u001b[39m__ror__\u001b[39;49m(pvalueish, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/apache_beam/transforms/ptransform.py:617\u001b[0m, in \u001b[0;36mPTransform.__ror__\u001b[0;34m(self, left, label)\u001b[0m\n\u001b[1;32m    615\u001b[0m pvalueish \u001b[39m=\u001b[39m _SetInputPValues()\u001b[39m.\u001b[39mvisit(pvalueish, replacements)\n\u001b[1;32m    616\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline \u001b[39m=\u001b[39m p\n\u001b[0;32m--> 617\u001b[0m result \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mapply(\u001b[39mself\u001b[39;49m, pvalueish, label)\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m deferred:\n\u001b[1;32m    619\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/apache_beam/pipeline.py:663\u001b[0m, in \u001b[0;36mPipeline.apply\u001b[0;34m(self, transform, pvalueish, label)\u001b[0m\n\u001b[1;32m    661\u001b[0m old_label, transform\u001b[39m.\u001b[39mlabel \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39mlabel, label\n\u001b[1;32m    662\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 663\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(transform, pvalueish)\n\u001b[1;32m    664\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    665\u001b[0m   transform\u001b[39m.\u001b[39mlabel \u001b[39m=\u001b[39m old_label\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/apache_beam/pipeline.py:709\u001b[0m, in \u001b[0;36mPipeline.apply\u001b[0;34m(self, transform, pvalueish, label)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[39mif\u001b[39;00m type_options\u001b[39m.\u001b[39mpipeline_type_check:\n\u001b[1;32m    707\u001b[0m   transform\u001b[39m.\u001b[39mtype_check_inputs(pvalueish)\n\u001b[0;32m--> 709\u001b[0m pvalueish_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunner\u001b[39m.\u001b[39;49mapply(transform, pvalueish, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_options)\n\u001b[1;32m    711\u001b[0m \u001b[39mif\u001b[39;00m type_options \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m type_options\u001b[39m.\u001b[39mpipeline_type_check:\n\u001b[1;32m    712\u001b[0m   transform\u001b[39m.\u001b[39mtype_check_outputs(pvalueish_result)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/apache_beam/runners/runner.py:185\u001b[0m, in \u001b[0;36mPipelineRunner.apply\u001b[0;34m(self, transform, input, options)\u001b[0m\n\u001b[1;32m    183\u001b[0m   m \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mapply_\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    184\u001b[0m   \u001b[39mif\u001b[39;00m m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mreturn\u001b[39;00m m(transform, \u001b[39minput\u001b[39;49m, options)\n\u001b[1;32m    186\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mExecution of [\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m] not implemented in runner \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (transform, \u001b[39mself\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/apache_beam/runners/runner.py:215\u001b[0m, in \u001b[0;36mPipelineRunner.apply_PTransform\u001b[0;34m(self, transform, input, options)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_PTransform\u001b[39m(\u001b[39mself\u001b[39m, transform, \u001b[39minput\u001b[39m, options):\n\u001b[1;32m    214\u001b[0m   \u001b[39m# The base case of apply is to call the transform's expand.\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m   \u001b[39mreturn\u001b[39;00m transform\u001b[39m.\u001b[39;49mexpand(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfx19/lib/python3.9/site-packages/tfx/components/example_gen/csv_example_gen/executor.py:183\u001b[0m, in \u001b[0;36m_CsvToExample.expand\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m    181\u001b[0m csv_files \u001b[39m=\u001b[39m fileio\u001b[39m.\u001b[39mglob(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_csv_pattern)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m csv_files:\n\u001b[0;32m--> 183\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mSplit pattern \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m does not match any files.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    184\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_csv_pattern))\n\u001b[1;32m    186\u001b[0m column_names \u001b[39m=\u001b[39m io_utils\u001b[39m.\u001b[39mload_csv_column_names(csv_files[\u001b[39m0\u001b[39m])\n\u001b[1;32m    187\u001b[0m \u001b[39mfor\u001b[39;00m csv_file \u001b[39min\u001b[39;00m csv_files[\u001b[39m1\u001b[39m:]:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Split pattern /tmp/tfx-data3e4ecj0p/* does not match any files."
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "PIPELINE_NAME = \"penguin-simple\"\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "\n",
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      module_file=_trainer_module_file,\n",
    "      serving_model_dir=SERVING_MODEL_DIR,\n",
    "      metadata_path=METADATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('tfx19')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2618b5bf89aa722145efd82c142152e44d6e48cd6f37b036b3fbc312f048a59a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
