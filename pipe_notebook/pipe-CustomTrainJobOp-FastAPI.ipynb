{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='jchavezar-demo' # Change it\n",
    "REGION='us-central1'\n",
    "TRAIN_IMAGE_URI=f'gcr.io/{PROJECT_ID}/custom_train:v1'\n",
    "PREDICT_IMAGE_URI=f'gcr.io/{PROJECT_ID}/custom_predict:v1'\n",
    "PIPELINE_ROOT_PATH='gs://vtx-root-path' # Change it\n",
    "AIP_STORAGE_URI='gs://vtx-artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr custom_train_job\n",
    "!mkdir custom_train_job\n",
    "!mkdir custom_train_job/train\n",
    "!mkdir custom_train_job/prediction\n",
    "!touch custom_train_job/train/__init__.py\n",
    "!touch custom_train_job/prediction/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Model code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Training and Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom_train_job/train/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_train_job/train/preprocess.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "data_uri = os.environ['AIP_STORAGE_URI']\n",
    "\n",
    "## Data Cleaning and Normalizating, exporting statistics.\n",
    "\n",
    "def train_pre_process(dataset):\n",
    "    dataset = dataset.dropna()\n",
    "    dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "    dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "    \n",
    "    train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    \n",
    "    train_stats = train_dataset.describe()\n",
    "    train_stats.pop('MPG')\n",
    "    train_stats = train_stats.transpose()\n",
    "    train_stats.to_csv(f'{data_uri}/mpg/stats.csv')\n",
    "    train_labels = train_dataset.pop('MPG')\n",
    "    test_labels = test_dataset.pop('MPG')\n",
    "    \n",
    "    def norm(x):\n",
    "        return (x - train_stats['mean'])/train_stats['std']\n",
    "    normed_train_data = norm(train_dataset)\n",
    "    normed_test_data = norm(test_dataset)\n",
    "\n",
    "    return normed_train_data, train_labels, normed_test_data, test_labels\n",
    "\n",
    "## Using training statistics to equals normalization.\n",
    "\n",
    "def pred_data_process(data: list):\n",
    "    column_names = ['Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin']\n",
    "    dataset = pd.DataFrame([data], columns=column_names)\n",
    "\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    if (dataset['Origin'] == 1).any():\n",
    "        dataset = dataset.drop(columns=['Origin'])\n",
    "        dataset['Europe'] = 0\n",
    "        dataset['Japan'] = 0\n",
    "        dataset['USA'] = 1\n",
    "\n",
    "    elif (dataset['Origin'].any == 2).any():\n",
    "        dataset = dataset.drop(columns=['Origin'])\n",
    "        dataset['Europe'] = 1\n",
    "        dataset['Japan'] = 0\n",
    "        dataset['USA'] = 0\n",
    "\n",
    "    elif (dataset['Origin'] == 3).any():\n",
    "        dataset = dataset.drop(columns=['Origin'])\n",
    "        dataset['Europe'] = 0\n",
    "        dataset['Japan'] = 1\n",
    "        dataset['USA'] = 0\n",
    "\n",
    "    ## Train stats\n",
    "    train_stats = pd.read_csv(f'{data_uri}/mpg/stats.csv', index_col=[0])\n",
    "    \n",
    "    def norm(x):\n",
    "        return (x - train_stats['mean'])/train_stats['std']\n",
    "    \n",
    "    return norm(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp custom_train_job/train/preprocess.py custom_train_job/prediction/preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_train_job/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_train_job/train/train.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model_uri = os.environ['AIP_STORAGE_URI']\n",
    "\n",
    "def build_model(train_data):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=[len(train_data.keys())]),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    \n",
    "    model.compile(loss='mse',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['mae', 'mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(train_data, train_labels, epochs: int = 1000):\n",
    "    \n",
    "    print('[INFO] ------ Building Model Layers', file=sys.stderr)\n",
    "    model = build_model(train_data)\n",
    "    epochs = epochs\n",
    "    \n",
    "    # The patience parameter is the amount of epochs to check for improvement\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    print('[INFO] ------ Iterations / Training', file=sys.stderr)\n",
    "    early_history = model.fit(train_data, train_labels, \n",
    "        epochs=epochs, validation_split = 0.2, \n",
    "        callbacks=[early_stop])\n",
    "    \n",
    "    print('[INFO] ------ Saving Model', file=sys.stderr)\n",
    "    model.save(f'{model_uri}/mpg/model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom_train_job/train/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_train_job/train/main.py\n",
    "\n",
    "import sys\n",
    "import preprocess\n",
    "import train\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight','Acceleration', 'Model Year', 'Origin']\n",
    "dataset = pd.read_csv(dataset_path, names=column_names, na_values = \"?\", comment='\\t',sep=\" \", skipinitialspace=True)\n",
    "\n",
    "## Clean, Normalize and Split Data\n",
    "\n",
    "print('[INFO] ------ Preparing Data', file=sys.stderr)\n",
    "train_data, train_labels, test_data, test_labels = preprocess.train_pre_process(dataset)\n",
    "\n",
    "## Train model and save it in Google Cloud Storage\n",
    "\n",
    "print('[INFO] ------ Training Model', file=sys.stderr)\n",
    "train.train_model(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom_train_job/train/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_train_job/train/Dockerfile\n",
    "\n",
    "FROM python:latest\n",
    "\n",
    "RUN python -m pip install --upgrade pip\n",
    "RUN pip install pandas gcsfs tensorflow\n",
    "COPY / /trainer\n",
    "\n",
    "CMD [\"python\", \"trainer/main.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 5 file(s) totalling 4.1 KiB before compression.\n",
      "Uploading tarball of [custom_train_job/train/.] to [gs://jchavezar-demo_cloudbuild/source/1662558550.233355-aff16ca48e1e41eeb67c1f2824ed0651.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/7bad8ad1-ab11-4457-a04a-9e967208da40].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/7bad8ad1-ab11-4457-a04a-9e967208da40?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"7bad8ad1-ab11-4457-a04a-9e967208da40\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1662558550.233355-aff16ca48e1e41eeb67c1f2824ed0651.tgz#1662558550877089\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1662558550.233355-aff16ca48e1e41eeb67c1f2824ed0651.tgz#1662558550877089...\n",
      "/ [1 files][  1.7 KiB/  1.7 KiB]                                                \n",
      "Operation completed over 1 objects/1.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  8.704kB\n",
      "Step 1/5 : FROM python:latest\n",
      "latest: Pulling from library/python\n",
      "Digest: sha256:745efdfb7e4aac9a8422bd8c62d8bc35a693e8979a240d29677cb03e6aa91052\n",
      "Status: Downloaded newer image for python:latest\n",
      " ---> d25a66380b10\n",
      "Step 2/5 : RUN python -m pip install --upgrade pip\n",
      " ---> Running in e95d30330969\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (22.2.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 29.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.2.1\n",
      "    Uninstalling pip-22.2.1:\n",
      "      Successfully uninstalled pip-22.2.1\n",
      "Successfully installed pip-22.2.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container e95d30330969\n",
      " ---> 73e37265a2fa\n",
      "Step 3/5 : RUN pip install pandas gcsfs tensorflow\n",
      " ---> Running in e242128e9dc4\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 52.8 MB/s eta 0:00:00\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2022.8.2-py2.py3-none-any.whl (25 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 578.0/578.0 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 500.6/500.6 kB 62.0 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.21.0\n",
      "  Downloading numpy-1.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/17.0 MB 69.2 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.8.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 37.0 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 kB 18.6 MB/s eta 0:00:00\n",
      "Collecting decorator>4.1.2\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.2/167.2 kB 25.6 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.5.2-py2.py3-none-any.whl (19 kB)\n",
      "Collecting fsspec==2022.8.2\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.8/140.8 kB 21.2 MB/s eta 0:00:00\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 93.4 MB/s eta 0:00:00\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 10.8 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 126.0 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 86.4 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 438.7/438.7 kB 57.7 MB/s eta 0:00:00\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 kB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow) (63.2.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 57.4 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 15.5 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 58.8 MB/s eta 0:00:00\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.48.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 90.9 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 12.2 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 119.6 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.4/123.4 kB 24.0 MB/s eta 0:00:00\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 109.2 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 8.7 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 13.4 MB/s eta 0:00:00\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 11.5 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 25.1 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (263 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.0/264.0 kB 39.9 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 22.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 30.9 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 67.4 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.7/232.7 kB 38.6 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 19.0 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 87.5 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.2/160.2 kB 29.2 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 12.7 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.4/140.4 kB 28.2 MB/s eta 0:00:00\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.10.0-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 22.9 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 kB 14.6 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 19.3 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.7/211.7 kB 37.2 MB/s eta 0:00:00\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.9.0-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.1/115.1 kB 22.2 MB/s eta 0:00:00\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.6/114.6 kB 22.8 MB/s eta 0:00:00\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 16.0 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.5/151.5 kB 26.7 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=4f88277161d352b192c51992b5c26c22aafad1bb2afebc3533f417c090a70ea5\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
      "Successfully built termcolor\n",
      "Installing collected packages: termcolor, tensorboard-plugin-wit, pytz, pyasn1, libclang, keras, flatbuffers, wrapt, urllib3, typing-extensions, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, rsa, pyparsing, pyasn1-modules, protobuf, oauthlib, numpy, multidict, MarkupSafe, markdown, idna, google-crc32c, gast, fsspec, frozenlist, decorator, charset-normalizer, certifi, cachetools, attrs, async-timeout, absl-py, yarl, werkzeug, requests, python-dateutil, packaging, opt-einsum, keras-preprocessing, h5py, grpcio, googleapis-common-protos, google-resumable-media, google-pasta, google-auth, astunparse, aiosignal, requests-oauthlib, pandas, google-api-core, aiohttp, google-cloud-core, google-auth-oauthlib, tensorboard, google-cloud-storage, tensorflow, gcsfs\n",
      "Successfully installed MarkupSafe-2.1.1 absl-py-1.2.0 aiohttp-3.8.1 aiosignal-1.2.0 astunparse-1.6.3 async-timeout-4.0.2 attrs-22.1.0 cachetools-5.2.0 certifi-2022.6.15 charset-normalizer-2.1.1 decorator-5.1.1 flatbuffers-2.0.7 frozenlist-1.3.1 fsspec-2022.8.2 gast-0.4.0 gcsfs-2022.8.2 google-api-core-2.8.2 google-auth-2.11.0 google-auth-oauthlib-0.4.6 google-cloud-core-2.3.2 google-cloud-storage-2.5.0 google-crc32c-1.5.0 google-pasta-0.2.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 grpcio-1.48.1 h5py-3.7.0 idna-3.3 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 multidict-6.0.2 numpy-1.23.2 oauthlib-3.2.0 opt-einsum-3.3.0 packaging-21.3 pandas-1.4.4 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2022.2.1 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 six-1.16.0 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 typing-extensions-4.3.0 urllib3-1.26.12 werkzeug-2.2.2 wrapt-1.14.1 yarl-1.8.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container e242128e9dc4\n",
      " ---> 11cec340c5f7\n",
      "Step 4/5 : COPY / /trainer\n",
      " ---> 0c8edc3ef093\n",
      "Step 5/5 : CMD [\"python\", \"trainer/main.py\"]\n",
      " ---> Running in d97343bb8b69\n",
      "Removing intermediate container d97343bb8b69\n",
      " ---> 8af8a97995fb\n",
      "Successfully built 8af8a97995fb\n",
      "Successfully tagged gcr.io/jchavezar-demo/custom_train:v1\n",
      "PUSH\n",
      "Pushing gcr.io/jchavezar-demo/custom_train:v1\n",
      "The push refers to repository [gcr.io/jchavezar-demo/custom_train]\n",
      "da7ad933c0f8: Preparing\n",
      "98c5968107a4: Preparing\n",
      "23d991670f06: Preparing\n",
      "bfc1deb8136e: Preparing\n",
      "1f123186824c: Preparing\n",
      "3d6eb1152931: Preparing\n",
      "100796cdf3b1: Preparing\n",
      "54acb5a6fa0b: Preparing\n",
      "8d51c618126f: Preparing\n",
      "9ff6e4d46744: Preparing\n",
      "a89d1d47b5a1: Preparing\n",
      "655ed1b7a428: Preparing\n",
      "54acb5a6fa0b: Waiting\n",
      "8d51c618126f: Waiting\n",
      "9ff6e4d46744: Waiting\n",
      "a89d1d47b5a1: Waiting\n",
      "655ed1b7a428: Waiting\n",
      "3d6eb1152931: Waiting\n",
      "100796cdf3b1: Waiting\n",
      "1f123186824c: Layer already exists\n",
      "bfc1deb8136e: Layer already exists\n",
      "3d6eb1152931: Layer already exists\n",
      "100796cdf3b1: Layer already exists\n",
      "54acb5a6fa0b: Layer already exists\n",
      "8d51c618126f: Layer already exists\n",
      "9ff6e4d46744: Layer already exists\n",
      "a89d1d47b5a1: Layer already exists\n",
      "655ed1b7a428: Layer already exists\n",
      "da7ad933c0f8: Pushed\n",
      "23d991670f06: Pushed\n",
      "98c5968107a4: Pushed\n",
      "v1: digest: sha256:347b0abb50037cc9173b990aa47eb0c1a6d22b24b31aa1263e0a160a86ef8a26 size: 2851\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                 STATUS\n",
      "7bad8ad1-ab11-4457-a04a-9e967208da40  2022-09-07T13:49:11+00:00  2M33S     gs://jchavezar-demo_cloudbuild/source/1662558550.233355-aff16ca48e1e41eeb67c1f2824ed0651.tgz  gcr.io/jchavezar-demo/custom_train:v1  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit -t $TRAIN_IMAGE_URI custom_train_job/train/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Serving with Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_train_job/prediction/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_train_job/prediction/main.py\n",
    "\n",
    "from fastapi import Request, FastAPI\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import preprocess\n",
    "import sys\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "model_uri=os.environ['AIP_STORAGE_URI']\n",
    "print(f'[INFO] ------ {model_uri}', file=sys.stderr)\n",
    "model = tf.keras.models.load_model(f'{model_uri}/mpg/model')\n",
    "\n",
    "@app.get('/')\n",
    "def get_root():\n",
    "    return {'message': 'Welcome mpg API: miles per gallon prediction'}\n",
    "\n",
    "@app.get('/health_check')\n",
    "def health():\n",
    "    return 200\n",
    "\n",
    "if os.environ.get('AIP_PREDICT_ROUTE') is not None:\n",
    "    method = os.environ['AIP_PREDICT_ROUTE']\n",
    "else:\n",
    "    method = '/predict'\n",
    "\n",
    "@app.post(method)\n",
    "async def predict(request: Request):\n",
    "    print(\"----------------- PREDICTING -----------------\")\n",
    "    body = await request.json()\n",
    "    instances = body[\"instances\"]\n",
    "    norm_data = preprocess.pred_data_process(instances)\n",
    "    outputs = model.predict(norm_data)\n",
    "    response = outputs.tolist()\n",
    "    print(\"----------------- OUTPUTS -----------------\")\n",
    "    return {\"predictions\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Container Image for Serving/Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_train_job/prediction/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_train_job/prediction/Dockerfile\n",
    "\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
    "\n",
    "COPY / /app\n",
    "WORKDIR /app\n",
    "RUN python -m pip install --upgrade pip\n",
    "RUN pip install pandas gcsfs tensorflow\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
    "\n",
    "EXPOSE 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 3.1 KiB before compression.\n",
      "Uploading tarball of [custom_train_job/prediction/.] to [gs://jchavezar-demo_cloudbuild/source/1662564407.842171-434467847fe845f5bad17af813209904.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/219ce390-6e0f-4654-aa5c-db3cb1e8502d].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/219ce390-6e0f-4654-aa5c-db3cb1e8502d?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"219ce390-6e0f-4654-aa5c-db3cb1e8502d\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1662564407.842171-434467847fe845f5bad17af813209904.tgz#1662564408309611\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1662564407.842171-434467847fe845f5bad17af813209904.tgz#1662564408309611...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/7 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
      "python3.7: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
      "df5590a8898b: Pulling fs layer\n",
      "705bb4cb554e: Pulling fs layer\n",
      "519df5fceacd: Pulling fs layer\n",
      "ccc287cbeddc: Pulling fs layer\n",
      "e3f8e6af58ed: Pulling fs layer\n",
      "aebed27b2d86: Pulling fs layer\n",
      "3b81dff756ff: Pulling fs layer\n",
      "6e4f59a23b58: Pulling fs layer\n",
      "5cdb61eb416d: Pulling fs layer\n",
      "415755650c07: Pulling fs layer\n",
      "82af7648a68a: Pulling fs layer\n",
      "697e3cdf6fdb: Pulling fs layer\n",
      "8d414b25a011: Pulling fs layer\n",
      "3e3cfa737f56: Pulling fs layer\n",
      "1595db7eb9b1: Pulling fs layer\n",
      "d2d3fcd4b870: Pulling fs layer\n",
      "35f87fd78dc8: Pulling fs layer\n",
      "146e266752fc: Pulling fs layer\n",
      "57df244389c0: Pulling fs layer\n",
      "e8391aa94779: Pulling fs layer\n",
      "ccc287cbeddc: Waiting\n",
      "e3f8e6af58ed: Waiting\n",
      "aebed27b2d86: Waiting\n",
      "3b81dff756ff: Waiting\n",
      "6e4f59a23b58: Waiting\n",
      "5cdb61eb416d: Waiting\n",
      "415755650c07: Waiting\n",
      "82af7648a68a: Waiting\n",
      "697e3cdf6fdb: Waiting\n",
      "8d414b25a011: Waiting\n",
      "3e3cfa737f56: Waiting\n",
      "1595db7eb9b1: Waiting\n",
      "d2d3fcd4b870: Waiting\n",
      "35f87fd78dc8: Waiting\n",
      "146e266752fc: Waiting\n",
      "57df244389c0: Waiting\n",
      "e8391aa94779: Waiting\n",
      "705bb4cb554e: Download complete\n",
      "519df5fceacd: Verifying Checksum\n",
      "519df5fceacd: Download complete\n",
      "df5590a8898b: Verifying Checksum\n",
      "df5590a8898b: Download complete\n",
      "aebed27b2d86: Verifying Checksum\n",
      "aebed27b2d86: Download complete\n",
      "ccc287cbeddc: Verifying Checksum\n",
      "ccc287cbeddc: Download complete\n",
      "6e4f59a23b58: Verifying Checksum\n",
      "6e4f59a23b58: Download complete\n",
      "3b81dff756ff: Verifying Checksum\n",
      "3b81dff756ff: Download complete\n",
      "415755650c07: Verifying Checksum\n",
      "415755650c07: Download complete\n",
      "5cdb61eb416d: Verifying Checksum\n",
      "5cdb61eb416d: Download complete\n",
      "697e3cdf6fdb: Verifying Checksum\n",
      "697e3cdf6fdb: Download complete\n",
      "82af7648a68a: Verifying Checksum\n",
      "82af7648a68a: Download complete\n",
      "8d414b25a011: Verifying Checksum\n",
      "8d414b25a011: Download complete\n",
      "3e3cfa737f56: Verifying Checksum\n",
      "3e3cfa737f56: Download complete\n",
      "1595db7eb9b1: Download complete\n",
      "d2d3fcd4b870: Verifying Checksum\n",
      "d2d3fcd4b870: Download complete\n",
      "35f87fd78dc8: Verifying Checksum\n",
      "35f87fd78dc8: Download complete\n",
      "146e266752fc: Verifying Checksum\n",
      "146e266752fc: Download complete\n",
      "e8391aa94779: Verifying Checksum\n",
      "e8391aa94779: Download complete\n",
      "57df244389c0: Verifying Checksum\n",
      "57df244389c0: Download complete\n",
      "e3f8e6af58ed: Verifying Checksum\n",
      "e3f8e6af58ed: Download complete\n",
      "df5590a8898b: Pull complete\n",
      "705bb4cb554e: Pull complete\n",
      "519df5fceacd: Pull complete\n",
      "ccc287cbeddc: Pull complete\n",
      "e3f8e6af58ed: Pull complete\n",
      "aebed27b2d86: Pull complete\n",
      "3b81dff756ff: Pull complete\n",
      "6e4f59a23b58: Pull complete\n",
      "5cdb61eb416d: Pull complete\n",
      "415755650c07: Pull complete\n",
      "82af7648a68a: Pull complete\n",
      "697e3cdf6fdb: Pull complete\n",
      "8d414b25a011: Pull complete\n",
      "3e3cfa737f56: Pull complete\n",
      "1595db7eb9b1: Pull complete\n",
      "d2d3fcd4b870: Pull complete\n",
      "35f87fd78dc8: Pull complete\n",
      "146e266752fc: Pull complete\n",
      "57df244389c0: Pull complete\n",
      "e8391aa94779: Pull complete\n",
      "Digest: sha256:f5770422a8875fe3d677e1bda724eeba332b53b0a348a9cdde854ba7136c66be\n",
      "Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
      " ---> bc3531b1f244\n",
      "Step 2/7 : COPY / /app\n",
      " ---> 9aec4a0de0f5\n",
      "Step 3/7 : WORKDIR /app\n",
      " ---> Running in f576014071ec\n",
      "Removing intermediate container f576014071ec\n",
      " ---> 68a009354b0e\n",
      "Step 4/7 : RUN python -m pip install --upgrade pip\n",
      " ---> Running in 2d77b411bbcb\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-22.2.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 2d77b411bbcb\n",
      " ---> 18c6cd6f46b1\n",
      "Step 5/7 : RUN pip install pandas gcsfs tensorflow\n",
      " ---> Running in 2b0635e1b4e4\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 42.9 MB/s eta 0:00:00\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2022.8.2-py2.py3-none-any.whl (25 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 578.0/578.0 MB 2.3 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.17.3\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 32.1 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 26.6 MB/s eta 0:00:00\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 500.6/500.6 kB 34.7 MB/s eta 0:00:00\n",
      "Collecting decorator>4.1.2\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 kB 15.6 MB/s eta 0:00:00\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 8.2 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.5.2-py2.py3-none-any.whl (19 kB)\n",
      "Collecting fsspec==2022.8.2\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.8/140.8 kB 20.5 MB/s eta 0:00:00\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.2/167.2 kB 20.5 MB/s eta 0:00:00\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 52.3 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 438.7/438.7 kB 38.4 MB/s eta 0:00:00\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 52.2 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 10.9 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.48.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 55.6 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/4.1 MB 58.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 65.8 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 53.3 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 46.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from tensorflow) (57.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.4/123.4 kB 17.0 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 kB 6.0 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 46.2 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 7.9 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.2/75.2 kB 10.8 MB/s eta 0:00:00\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.8/94.8 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.0/148.0 kB 19.5 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 231.3/231.3 kB 24.7 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 18.8 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.7/232.7 kB 25.4 MB/s eta 0:00:00\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 13.6 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 48.9 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 46.4 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.2/160.2 kB 22.1 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.4/140.4 kB 20.3 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 kB 10.4 MB/s eta 0:00:00\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.10.0-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 14.7 MB/s eta 0:00:00\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 12.7 MB/s eta 0:00:00\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.9.0-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.1/115.1 kB 15.6 MB/s eta 0:00:00\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.6/114.6 kB 15.6 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.7/211.7 kB 26.6 MB/s eta 0:00:00\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.8.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 7.7 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.5/151.5 kB 15.5 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.6.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=161eee566ed18156ed83d80f61d61f1a16b4cb21576dc8c00224141978ec8518\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built termcolor\n",
      "Installing collected packages: termcolor, tensorboard-plugin-wit, pytz, pyasn1, libclang, keras, flatbuffers, wrapt, urllib3, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, rsa, pyparsing, pyasn1-modules, protobuf, oauthlib, numpy, multidict, MarkupSafe, idna, google-crc32c, gast, fsspec, frozenlist, decorator, charset-normalizer, certifi, cachetools, attrs, asynctest, async-timeout, absl-py, yarl, werkzeug, requests, python-dateutil, packaging, opt-einsum, markdown, keras-preprocessing, h5py, grpcio, googleapis-common-protos, google-resumable-media, google-pasta, google-auth, astunparse, aiosignal, requests-oauthlib, pandas, google-api-core, aiohttp, google-cloud-core, google-auth-oauthlib, tensorboard, google-cloud-storage, tensorflow, gcsfs\n",
      "Successfully installed MarkupSafe-2.1.1 absl-py-1.2.0 aiohttp-3.8.1 aiosignal-1.2.0 astunparse-1.6.3 async-timeout-4.0.2 asynctest-0.13.0 attrs-22.1.0 cachetools-5.2.0 certifi-2022.6.15 charset-normalizer-2.1.1 decorator-5.1.1 flatbuffers-2.0.7 frozenlist-1.3.1 fsspec-2022.8.2 gast-0.4.0 gcsfs-2022.8.2 google-api-core-2.8.2 google-auth-2.11.0 google-auth-oauthlib-0.4.6 google-cloud-core-2.3.2 google-cloud-storage-2.5.0 google-crc32c-1.5.0 google-pasta-0.2.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 grpcio-1.48.1 h5py-3.7.0 idna-3.3 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 multidict-6.0.2 numpy-1.21.6 oauthlib-3.2.0 opt-einsum-3.3.0 packaging-21.3 pandas-1.3.5 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2022.2.1 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 six-1.16.0 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 urllib3-1.26.12 werkzeug-2.2.2 wrapt-1.14.1 yarl-1.8.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 2b0635e1b4e4\n",
      " ---> cee6a6242fb2\n",
      "Step 6/7 : CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
      " ---> Running in 1780b8e12f7f\n",
      "Removing intermediate container 1780b8e12f7f\n",
      " ---> 5f147c1c1400\n",
      "Step 7/7 : EXPOSE 8080\n",
      " ---> Running in 017daa36dd54\n",
      "Removing intermediate container 017daa36dd54\n",
      " ---> 506995c39591\n",
      "Successfully built 506995c39591\n",
      "Successfully tagged gcr.io/jchavezar-demo/custom_predict:v1\n",
      "PUSH\n",
      "Pushing gcr.io/jchavezar-demo/custom_predict:v1\n",
      "The push refers to repository [gcr.io/jchavezar-demo/custom_predict]\n",
      "c30c8720055e: Preparing\n",
      "6df4b4e8b40c: Preparing\n",
      "996a5f43b5fe: Preparing\n",
      "574be7dd3a19: Preparing\n",
      "4a24608ea27b: Preparing\n",
      "c660563b938b: Preparing\n",
      "e313cca5ff4a: Preparing\n",
      "ad6638039b02: Preparing\n",
      "4c503f7b7cec: Preparing\n",
      "49022fdb47b6: Preparing\n",
      "f4cc5099892d: Preparing\n",
      "471ef6255d6e: Preparing\n",
      "0f8cd2835d79: Preparing\n",
      "0616239ad62a: Preparing\n",
      "4eb436881a0f: Preparing\n",
      "7fe8548565b4: Preparing\n",
      "8364a493536b: Preparing\n",
      "c1792902851c: Preparing\n",
      "c272c95c3fb0: Preparing\n",
      "3054497613e6: Preparing\n",
      "d35dc7f4c79e: Preparing\n",
      "dabfe5b2ea81: Preparing\n",
      "5e6a409f30b6: Preparing\n",
      "c660563b938b: Waiting\n",
      "e313cca5ff4a: Waiting\n",
      "ad6638039b02: Waiting\n",
      "4c503f7b7cec: Waiting\n",
      "49022fdb47b6: Waiting\n",
      "f4cc5099892d: Waiting\n",
      "471ef6255d6e: Waiting\n",
      "0f8cd2835d79: Waiting\n",
      "0616239ad62a: Waiting\n",
      "4eb436881a0f: Waiting\n",
      "7fe8548565b4: Waiting\n",
      "8364a493536b: Waiting\n",
      "c1792902851c: Waiting\n",
      "c272c95c3fb0: Waiting\n",
      "3054497613e6: Waiting\n",
      "d35dc7f4c79e: Waiting\n",
      "dabfe5b2ea81: Waiting\n",
      "5e6a409f30b6: Waiting\n",
      "574be7dd3a19: Layer already exists\n",
      "4a24608ea27b: Layer already exists\n",
      "e313cca5ff4a: Layer already exists\n",
      "c660563b938b: Layer already exists\n",
      "ad6638039b02: Layer already exists\n",
      "4c503f7b7cec: Layer already exists\n",
      "49022fdb47b6: Layer already exists\n",
      "f4cc5099892d: Layer already exists\n",
      "471ef6255d6e: Layer already exists\n",
      "0f8cd2835d79: Layer already exists\n",
      "0616239ad62a: Layer already exists\n",
      "4eb436881a0f: Layer already exists\n",
      "7fe8548565b4: Layer already exists\n",
      "8364a493536b: Layer already exists\n",
      "c1792902851c: Layer already exists\n",
      "c272c95c3fb0: Layer already exists\n",
      "3054497613e6: Layer already exists\n",
      "dabfe5b2ea81: Layer already exists\n",
      "996a5f43b5fe: Pushed\n",
      "d35dc7f4c79e: Layer already exists\n",
      "5e6a409f30b6: Layer already exists\n",
      "6df4b4e8b40c: Pushed\n",
      "c30c8720055e: Pushed\n",
      "v1: digest: sha256:0e262eeec478365d38070ec13640c98e06f619c4c0937a16c45bdf9240e265f8 size: 5138\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                   STATUS\n",
      "219ce390-6e0f-4654-aa5c-db3cb1e8502d  2022-09-07T15:26:48+00:00  5M47S     gs://jchavezar-demo_cloudbuild/source/1662564407.842171-434467847fe845f5bad17af813209904.tgz  gcr.io/jchavezar-demo/custom_predict:v1  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit -t $PREDICT_IMAGE_URI custom_train_job/prediction/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Worker Specs\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\"\n",
    "        },\n",
    "        \"replica_count\": \"1\",\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE_URI,\n",
    "            \"env\": [\n",
    "                {\n",
    "                    \"name\": \"AIP_STORAGE_URI\",\n",
    "                    \"value\": AIP_STORAGE_URI\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import pipeline\n",
    "from kfp.v2.components import importer_node\n",
    "from google_cloud_pipeline_components import aiplatform as gcc\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "\n",
    "@pipeline(name='custom-train')\n",
    "def pipeline(\n",
    "    project_id: str\n",
    "):\n",
    "    train_job = CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        display_name='custom_train',\n",
    "        worker_pool_specs=worker_pool_specs\n",
    "    )\n",
    "    import_unmanaged_model_op = importer_node.importer(\n",
    "            artifact_uri=AIP_STORAGE_URI,\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": PREDICT_IMAGE_URI,\n",
    "                    \"env\": [\n",
    "                        {\n",
    "                            \"name\": \"PROJECT_ID\",\n",
    "                            \"value\": PROJECT_ID},\n",
    "                    ],\n",
    "                    \"predictRoute\": \"/predict\",\n",
    "                    \"healthRoute\": \"/health_check\",\n",
    "                    \"ports\": [\n",
    "                        {\n",
    "                            \"containerPort\": 8080\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "    ).after(train_job)\n",
    "    custom_model_upload_job = gcc.ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=\"customjob-model\",\n",
    "        unmanaged_container_model=import_unmanaged_model_op.outputs[\"artifact\"],\n",
    "        ).after(import_unmanaged_model_op)\n",
    "    endpoint_create_job = gcc.EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=\"cutomjob-endpoint\",\n",
    "    )\n",
    "            \n",
    "    custom_model_deploy_job = (gcc.ModelDeployOp(\n",
    "        model=custom_model_upload_job.outputs[\"model\"],\n",
    "        endpoint=endpoint_create_job.outputs[\"endpoint\"],\n",
    "        deployed_model_display_name=\"cutomjob-deploy\",\n",
    "        traffic_split={\"0\":\"100\"},\n",
    "        dedicated_resources_machine_type=\"n1-standard-2\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1\n",
    "        )).set_caching_options(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='custom_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/custom-train-20220907121011\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/custom-train-20220907121011')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-train-20220907121011?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"custom_train\",\n",
    "    template_path=\"custom_train.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID\n",
    "    },\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('gcp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61f821d259e852bb8dda541b337ba40be66c16e8431d3e97d4d2c7f8d54d4461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
