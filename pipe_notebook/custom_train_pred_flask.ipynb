{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a337da5c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This is just a bunch of snippets ordered to create a quick pipeline which demonstrates Vertex capabilities, the pipeline contains different technologies, frameworks, etc, like dask, rapids, docker for the training leveraging GPU and using Flask as webserver for online predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a784d-6402-442d-9a66-a47fa6a8f4ad",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0366ebbc-760e-4510-b176-c57a805ab6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "REGION = 'us-central1'\n",
    "CUSTOM_TRAIN_NAME = 'gpu_custom_job'\n",
    "PIPELINE_ROOT_PATH = 'gs://vtx-root-path'\n",
    "MODEL_FILE_BUCKET = 'gs://vtx-pipe-models'\n",
    "TRAINING_REPOSITORY = 'trainings'\n",
    "IMAGE_URI = f'us-central1-docker.pkg.dev/jchavezar-demo/{TRAINING_REPOSITORY}/train_gpu_xgb:latest'\n",
    "PREDICTION_REPOSITORY = 'predictions'\n",
    "PREDICTION_IMAGE_URI = f'us-central1-docker.pkg.dev/jchavezar-demo/{PREDICTION_REPOSITORY}/prediction:latest'\n",
    "ART_REG = IMAGE_URI.split('/')[0]\n",
    "DATASET_DISPLAY_NAME = 'covertype-4Mr'\n",
    "DATASET_SOURCE = 'gs://vtx-datasets-public/cover_type_4Mrows.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04690df7-d0e8-4a84-a118-b1c11c5c522e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42af723-702e-41a3-8b86-a8bc8deb6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e7aac3-79af-43a7-a876-deb02e6cf8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e88231a7-68ce-4e19-898b-f4966ed1ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir training/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfaa3fd-ae4d-4e5c-a09e-1f63ea433877",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch training/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2197d9e-6788-40d7-9ade-5a3495ba1004",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils for Storing Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3e6d22-d977-40da-8134-8947a37c5dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/trainer/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/trainer/utils.py\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def save_model(args):\n",
    "    \"\"\"Saves the model to Google Cloud Storage or local file system\n",
    "    Args:\n",
    "      args: contains name for saved model.\n",
    "    \"\"\"\n",
    "    scheme = 'gs://'\n",
    "    if args.job_dir.startswith(scheme):\n",
    "        print(f\"Reading input job_dir: {args.job_dir}\")\n",
    "        job_dir = args.job_dir.split(\"/\")\n",
    "        bucket_name = job_dir[2]\n",
    "        object_prefix = \"/\".join(job_dir[3:]).rstrip(\"/\")\n",
    "        print(f\"Reading object_prefix: {object_prefix}\")\n",
    "\n",
    "        if object_prefix:\n",
    "            model_path = '{}/{}'.format(object_prefix, \"xgboost\")\n",
    "        else:\n",
    "            model_path = '{}'.format(\"xgboost\")\n",
    "            \n",
    "        print(f\"The model path is {model_path}\")\n",
    "        bucket = storage.Client().bucket(bucket_name)    \n",
    "        local_path = os.path.join(\"/tmp\", \"xgboost\")\n",
    "        files = [f for f in os.listdir(local_path) if os.path.isfile(os.path.join(local_path, f))]\n",
    "        for file in files:\n",
    "            local_file = os.path.join(local_path, file)\n",
    "            blob = bucket.blob(\"/\".join([model_path, file]))\n",
    "            blob.upload_from_filename(local_file)\n",
    "        print(local_file)\n",
    "        print(f\"gs://{bucket_name}/{model_path}\")\n",
    "        print(f\"Saved model files in gs://{bucket_name}/{model_path}\")\n",
    "    else:\n",
    "        print(f\"Saved model files at {os.path.join('/tmp', args.model_name)}\")\n",
    "        print(f\"To save model files in GCS bucket, please specify job_dir starting with gs://\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1702b-45ec-4dca-8aaf-992886d3c3ca",
   "metadata": {},
   "source": [
    "## Training Code with Dask + CUDA (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e89b392d-c322-49d3-b801-639097275a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/trainer/task.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import dask_cudf\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "#import pickle\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset_source', dest='dataset',\n",
    "                    type=str,\n",
    "                    help='Dataset.')\n",
    "parser.add_argument(\n",
    "    '--job-dir',\n",
    "    default=os.getenv('AIP_MODEL_DIR'),\n",
    "    help='GCS location to export models')\n",
    "parser.add_argument(\n",
    "    '--model-name',\n",
    "    default=\"custom-train\",\n",
    "    help='The name of your saved model')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.info(f\"Importing dataset {args.dataset}\")\n",
    "df = dask_cudf.read_csv(args.dataset)\n",
    "\n",
    "logging.info(\"Cleaning and standarizing dataset\")\n",
    "df = df.dropna()\n",
    "\n",
    "logging.info(f\"Splitting dataset\")\n",
    "df_train, df_eval = df.random_split([0.8, 0.2], random_state=123)\n",
    "\n",
    "df_train_features= df_train.drop('Cover_Type', axis=1)\n",
    "df_eval_features= df_eval.drop('Cover_Type', axis=1)\n",
    "\n",
    "df_train_labels = df_train.pop('Cover_Type')\n",
    "df_eval_labels = df_eval.pop('Cover_Type')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import utils\n",
    "\n",
    "    logging.info(\"Creating dask cluster\")\n",
    "    cluster = LocalCUDACluster()\n",
    "    client = Client(cluster)\n",
    "    \n",
    "    logging.info(client)\n",
    "    \n",
    "    # X and y must be Dask dataframes or arrays\n",
    "    \n",
    "    print(xgb.__version__)\n",
    "\n",
    "    logging.info(\"Dataset for dask\")\n",
    "    dtrain = xgb.dask.DaskDMatrix(client, df_train_features, df_train_labels)\n",
    "    \n",
    "    logging.info(\"Dataset for dask\")\n",
    "    dvalid = xgb.dask.DaskDMatrix(client, df_eval_features, df_eval_labels)\n",
    "\n",
    "    logging.info(\"Training\")\n",
    "    output = xgb.dask.train(\n",
    "        client,\n",
    "        {\n",
    "            \"verbosity\": 2, \n",
    "            \"tree_method\": \"gpu_hist\", \n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"eval_metric\": [\"mlogloss\"],\n",
    "            \"num_class\": 8\n",
    "        },\n",
    "        dtrain,\n",
    "        num_boost_round=4,\n",
    "        evals=[(dvalid, \"valid1\")],\n",
    "        early_stopping_rounds=5\n",
    "    )\n",
    "    \n",
    "    # Saving models and exporting performance metrics\n",
    "    \n",
    "    df_eval_metrics = pd.DataFrame(output[\"history\"][\"valid1\"])\n",
    "    model = output[\"booster\"]\n",
    "    best_model = model[: model.best_iteration]\n",
    "    logging.info(f\"Best model: {best_model}\")\n",
    "    temp_dir = \"/tmp/xgboost\"\n",
    "    os.mkdir(temp_dir)\n",
    "    best_model.save_model(\"{}/{}\".format(temp_dir, args.model_name))\n",
    "    df_eval_metrics.to_json(\"{}/all_results.json\".format(temp_dir))\n",
    "\n",
    "    utils.save_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defad8e8-c925-4e38-8545-20bff43bb3bd",
   "metadata": {},
   "source": [
    "## Wrapping Code (Custom Container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee3403c9-df5c-44dd-b3ad-af71a7ff01de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/Dockerfile\n",
    "\n",
    "FROM rapidsai/rapidsai-nightly:22.04-cuda11.2-base-ubuntu20.04-py3.9\n",
    "\n",
    "RUN pip install google.cloud[storage] \\\n",
    "  && pip install gcsfs \\\n",
    "  && pip install xgboost --upgrade\n",
    "\n",
    "COPY trainer trainer/\n",
    "\n",
    "ENTRYPOINT [\"python\", \"trainer/task.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76450e-37a6-4f19-983b-8b49e0394fdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create: Repositories and Push Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3daff53-b1b1-4912-a584-f1a15fe6207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $TRAINING_REPOSITORY --location $REGION --repository-format docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2610c13-93bf-4a9a-9f47-90243303cf51",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit -t $IMAGE_URI training/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128fb06-e3d4-4775-89ee-1a09d5c33dd3",
   "metadata": {},
   "source": [
    "## Create: Vertex Pipe Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1afce7-600e-43f6-90e4-ddd1be60d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (component, Input, Output, Model, Metrics, Model)\n",
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "    ],\n",
    ")\n",
    "def get_train_job_details(\n",
    "    model_dir: str,\n",
    "    model_display_name: str,\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    eval_metric_key: str\n",
    "    ) -> NamedTuple(\n",
    "    \"Outputs\", [(\"eval_metric\", float)]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    metrics_uri = \"{}/model/xgboost/all_results.json\".format(model_dir)\n",
    "    metrics_df = pd.read_json(metrics_uri, typ=\"series\")\n",
    "    for k,v in metrics_df.items():\n",
    "        logging.info(f\"    {k} -> {v}\")\n",
    "        metrics.log_metric(k, min(v.values()))\n",
    "        \n",
    "    eval_metric = (min(metrics_df[eval_metric_key].values()) if eval_metric_key in metrics_df.keys() else None)\n",
    "    model.metadata[eval_metric_key] = eval_metric\n",
    "    outputs = namedtuple(\"Outputs\", [\"eval_metric\"])\n",
    "    \n",
    "    return outputs(eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29adaf9b-4066-44a0-b696-2e9f7439f1bb",
   "metadata": {},
   "source": [
    "## Create: Prediction Custom Container Vertex|Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3087f0-3e36-4e56-b073-60f930cf9161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/app.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from flask import Flask, request, Response, jsonify\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client(project=os.environ['PROJECT_ID'])\n",
    "\n",
    "# Model Download from gcs\n",
    "\n",
    "fname = \"model.json\"\n",
    "\n",
    "with open(fname, \"wb\") as model:\n",
    "    client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/{fname}\", model\n",
    "    )\n",
    "\n",
    "# Loading model\n",
    "print(\"Loading model from: {}\".format(fname))\n",
    "model = xgb.Booster(model_file=fname)\n",
    "\n",
    "# Creation of the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Flask route for Liveness checks\n",
    "@app.route(os.environ['AIP_HEALTH_ROUTE'])\n",
    "def isalive():\n",
    "    status_code = Response(status=200)\n",
    "    return status_code\n",
    "\n",
    "# Flask route for predictions\n",
    "@app.route(os.environ['AIP_PREDICT_ROUTE'],methods=['GET','POST'])\n",
    "def prediction():\n",
    "    _features = ['Id','Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "                          'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm','Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', \n",
    "                          'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9',\n",
    "                          'Soil_Type10','Soil_Type11','Soil_Type12','Soil_Type13','Soil_Type14','Soil_Type15','Soil_Type16','Soil_Type17','Soil_Type18','Soil_Type19', \n",
    "                          'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29',\n",
    "                          'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n",
    "    data = request.get_json(silent=True, force=True)\n",
    "    dmf = xgb.DMatrix(pd.DataFrame(data[\"instances\"], columns=_features))\n",
    "    response = pd.DataFrame(model.predict(dmf))\n",
    "    logging.info(f\"Response: {response}\")\n",
    "    return jsonify({\"Cover Type\": str(response.idxmax(axis=1)[0])})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa60ef-8088-447b-914f-812294877734",
   "metadata": {},
   "source": [
    "### Preparing Docker Container Declarative Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086026cc-fff9-454a-8608-cc413d222701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prediction/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/requirements.txt\n",
    "\n",
    "google-cloud-storage\n",
    "numpy\n",
    "pandas\n",
    "flask\n",
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05bfdf9f-f5f8-4e6e-bb1d-ae4dc335a0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/Dockerfile\n",
    "\n",
    "FROM python:3.7-buster\n",
    "\n",
    "RUN mkdir my-model\n",
    "\n",
    "COPY app.py ./app.py\n",
    "COPY requirements.txt ./requirements.txt\n",
    "RUN pip install -r requirements.txt \n",
    "\n",
    "# Flask Env Variable\n",
    "ENV FLASK_APP=app\n",
    "\n",
    "# Expose port 8080\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD flask run --host=0.0.0.0 --port=8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725956dc-2ee0-43cd-96bb-1f64796ddc44",
   "metadata": {},
   "source": [
    "### Push Container Image to Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37a7f8-8eb6-4066-867f-d2c868cdd94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $PREDICTION_REPOSITORY --location $REGION --repository-format docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34444603-e3ae-4d4a-85ca-ab8e7017fb15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $PREDICTION_IMAGE_URI prediction/. --timeout 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f5e54-1647-42e2-b15c-235ae18b163e",
   "metadata": {},
   "source": [
    "## Create: Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afac2a5d-347d-4d73-80a5-12dc9d08d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import pipeline, Condition\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components import aiplatform as gcc\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp as custom_job\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from kfp.v2.components import importer_node\n",
    "\n",
    "\n",
    "@pipeline(name='dask-gpu-1')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    custom_train_name: str,\n",
    "    region: str,\n",
    "    eval_acc_threshold: float,\n",
    "    eval_metric_key: str,\n",
    "    model_file_bucket: str,\n",
    "):\n",
    "    worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"a2-highgpu-1g\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_A100\",\n",
    "            \"accelerator_count\": 1\n",
    "        },\n",
    "        \"replica_count\": \"1\",\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"env\": [{\"name\": \"AIP_TRAINING_DATA_URI\", \"value\":'test'}],\n",
    "            \"command\": [\n",
    "                \"python\",\n",
    "                \"trainer/task.py\"\n",
    "            ],\n",
    "            \"args\": [\n",
    "                \"--dataset_source\", \"gs://vtx-datasets-public/cover_type_4Mrows.csv\",\n",
    "                \"--model-name\", \"model.json\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    ]\n",
    "    train_with_cpu_task = custom_job(\n",
    "        project=project_id,\n",
    "        display_name=custom_train_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_directory=model_file_bucket\n",
    "    )\n",
    "    get_train_details_task = get_train_job_details(\n",
    "        model_dir=MODEL_FILE_BUCKET,\n",
    "        model_display_name=\"xgboost-dask\",\n",
    "        eval_metric_key=eval_metric_key, # mlogloss\n",
    "    ).after(train_with_cpu_task)\n",
    "    \n",
    "    with Condition(\n",
    "        get_train_details_task.outputs[\"eval_metric\"] > eval_acc_threshold,\n",
    "        name=\"model-deploy-decision\",\n",
    "    ):\n",
    "        import_unmanaged_model_op = importer_node.importer(\n",
    "            artifact_uri=\"gs://vtx-models/model/xgboost\",\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": PREDICTION_IMAGE_URI,\n",
    "                    \"env\": [\n",
    "                        {\n",
    "                            \"name\": \"PROJECT_ID\",\n",
    "                            \"value\": PROJECT_ID},\n",
    "                    ],\n",
    "                    \"predictRoute\": \"/predict\",\n",
    "                    \"healthRoute\": \"/health\",\n",
    "                    \"ports\": [\n",
    "                        {\n",
    "                            \"containerPort\": 8080\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        custom_model_upload_job = gcc.ModelUploadOp(\n",
    "            project=project_id,\n",
    "            display_name=\"xgb-model\",\n",
    "            unmanaged_container_model=import_unmanaged_model_op.outputs[\"artifact\"],\n",
    "        ).after(import_unmanaged_model_op)\n",
    "        \n",
    "        endpoint_create_job = gcc.EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            display_name=\"pipelines-created-endpoint\",\n",
    "        )\n",
    "        \n",
    "        custom_model_deploy_job = (gcc.ModelDeployOp(\n",
    "            model=custom_model_upload_job.outputs[\"model\"],\n",
    "            endpoint=endpoint_create_job.outputs[\"endpoint\"],\n",
    "            deployed_model_display_name=\"xgboost_model_end\",\n",
    "            traffic_split={\"0\":\"100\"},\n",
    "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1\n",
    "        )).set_caching_options(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8c3de-97f0-410e-8293-cd3d77bf70f1",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa4335c-36f7-438e-9d46-c22ab5b1e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='dask_cpu.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572bb69-8054-46bd-939b-252ee0b3b7ec",
   "metadata": {},
   "source": [
    "## Run Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c998a74-c573-4732-bc80-6ab24877e38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/dask-gpu-1-20220801130945\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/dask-gpu-1-20220801130945')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/dask-gpu-1-20220801130945?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"dask_cpu\",\n",
    "    template_path=\"dask_cpu.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        'custom_train_name': CUSTOM_TRAIN_NAME,\n",
    "        'region': REGION,\n",
    "        'eval_acc_threshold': 0.5,\n",
    "        'eval_metric_key': 'mlogloss', # mlogloss\n",
    "        'model_file_bucket': MODEL_FILE_BUCKET,\n",
    "    },\n",
    ")\n",
    "\n",
    "job.submit(service_account='vtx-pipe@jchavezar-demo.iam.gserviceaccount.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a13c1-fe16-48f4-85e7-cbeb029f7697",
   "metadata": {},
   "source": [
    "<img src=\"images/vertex-pipe-gpu.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562658e2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('gcp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "61f821d259e852bb8dda541b337ba40be66c16e8431d3e97d4d2c7f8d54d4461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
