{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a337da5c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This is just a bunch of snippets ordered to create a quick pipeline which demonstrates Vertex capabilities, the pipeline contains different technologies, frameworks, etc, like dask, rapids, docker for the training leveraging GPU and using Flask as webserver for online predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a784d-6402-442d-9a66-a47fa6a8f4ad",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0366ebbc-760e-4510-b176-c57a805ab6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "REGION = 'us-central1'\n",
    "CUSTOM_TRAIN_NAME = 'gpu_custom_job'\n",
    "PIPELINE_ROOT_PATH = 'gs://vtx-root-path'\n",
    "MODEL_FILE_BUCKET = 'gs://vtx-pipe-models'\n",
    "TRAINING_REPOSITORY = 'trainings'\n",
    "IMAGE_URI = f'us-central1-docker.pkg.dev/jchavezar-demo/{TRAINING_REPOSITORY}/train_gpu_xgb:latest'\n",
    "PREDICTION_REPOSITORY = 'predictions'\n",
    "PREDICTION_IMAGE_URI = f'us-central1-docker.pkg.dev/jchavezar-demo/{PREDICTION_REPOSITORY}/prediction:latest'\n",
    "ART_REG = IMAGE_URI.split('/')[0]\n",
    "DATASET_DISPLAY_NAME = 'covertype-4Mr'\n",
    "DATASET_SOURCE = 'gs://vtx-datasets-public/cover_type_4Mrows.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04690df7-d0e8-4a84-a118-b1c11c5c522e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42af723-702e-41a3-8b86-a8bc8deb6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e7aac3-79af-43a7-a876-deb02e6cf8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e88231a7-68ce-4e19-898b-f4966ed1ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir training/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfaa3fd-ae4d-4e5c-a09e-1f63ea433877",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch training/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2197d9e-6788-40d7-9ade-5a3495ba1004",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils for Storing Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3e6d22-d977-40da-8134-8947a37c5dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/trainer/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/trainer/utils.py\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def save_model(args):\n",
    "    \"\"\"Saves the model to Google Cloud Storage or local file system\n",
    "    Args:\n",
    "      args: contains name for saved model.\n",
    "    \"\"\"\n",
    "    scheme = 'gs://'\n",
    "    if args.job_dir.startswith(scheme):\n",
    "        print(f\"Reading input job_dir: {args.job_dir}\")\n",
    "        job_dir = args.job_dir.split(\"/\")\n",
    "        bucket_name = job_dir[2]\n",
    "        object_prefix = \"/\".join(job_dir[3:]).rstrip(\"/\")\n",
    "        print(f\"Reading object_prefix: {object_prefix}\")\n",
    "\n",
    "        if object_prefix:\n",
    "            model_path = '{}/{}'.format(object_prefix, \"xgboost\")\n",
    "        else:\n",
    "            model_path = '{}'.format(\"xgboost\")\n",
    "            \n",
    "        print(f\"The model path is {model_path}\")\n",
    "        bucket = storage.Client().bucket(bucket_name)    \n",
    "        local_path = os.path.join(\"/tmp\", \"xgboost\")\n",
    "        files = [f for f in os.listdir(local_path) if os.path.isfile(os.path.join(local_path, f))]\n",
    "        for file in files:\n",
    "            local_file = os.path.join(local_path, file)\n",
    "            blob = bucket.blob(\"/\".join([model_path, file]))\n",
    "            blob.upload_from_filename(local_file)\n",
    "        print(local_file)\n",
    "        print(f\"gs://{bucket_name}/{model_path}\")\n",
    "        print(f\"Saved model files in gs://{bucket_name}/{model_path}\")\n",
    "    else:\n",
    "        print(f\"Saved model files at {os.path.join('/tmp', args.model_name)}\")\n",
    "        print(f\"To save model files in GCS bucket, please specify job_dir starting with gs://\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1702b-45ec-4dca-8aaf-992886d3c3ca",
   "metadata": {},
   "source": [
    "## Training Code with Dask + CUDA (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e89b392d-c322-49d3-b801-639097275a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/trainer/task.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import dask_cudf\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "#import pickle\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset_source', dest='dataset',\n",
    "                    type=str,\n",
    "                    help='Dataset.')\n",
    "parser.add_argument(\n",
    "    '--job-dir',\n",
    "    default=os.getenv('AIP_MODEL_DIR'),\n",
    "    help='GCS location to export models')\n",
    "parser.add_argument(\n",
    "    '--model-name',\n",
    "    default=\"custom-train\",\n",
    "    help='The name of your saved model')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.info(f\"Importing dataset {args.dataset}\")\n",
    "df = dask_cudf.read_csv(args.dataset)\n",
    "\n",
    "logging.info(\"Cleaning and standarizing dataset\")\n",
    "df = df.dropna()\n",
    "\n",
    "logging.info(f\"Splitting dataset\")\n",
    "df_train, df_eval = df.random_split([0.8, 0.2], random_state=123)\n",
    "\n",
    "df_train_features= df_train.drop('Cover_Type', axis=1)\n",
    "df_eval_features= df_eval.drop('Cover_Type', axis=1)\n",
    "\n",
    "df_train_labels = df_train.pop('Cover_Type')\n",
    "df_eval_labels = df_eval.pop('Cover_Type')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import utils\n",
    "\n",
    "    logging.info(\"Creating dask cluster\")\n",
    "    cluster = LocalCUDACluster()\n",
    "    client = Client(cluster)\n",
    "    \n",
    "    logging.info(client)\n",
    "    \n",
    "    # X and y must be Dask dataframes or arrays\n",
    "    \n",
    "    print(xgb.__version__)\n",
    "\n",
    "    logging.info(\"Dataset for dask\")\n",
    "    dtrain = xgb.dask.DaskDMatrix(client, df_train_features, df_train_labels)\n",
    "    \n",
    "    logging.info(\"Dataset for dask\")\n",
    "    dvalid = xgb.dask.DaskDMatrix(client, df_eval_features, df_eval_labels)\n",
    "\n",
    "    logging.info(\"Training\")\n",
    "    output = xgb.dask.train(\n",
    "        client,\n",
    "        {\n",
    "            \"verbosity\": 2, \n",
    "            \"tree_method\": \"gpu_hist\", \n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"eval_metric\": [\"mlogloss\"],\n",
    "            \"num_class\": 8\n",
    "        },\n",
    "        dtrain,\n",
    "        num_boost_round=4,\n",
    "        evals=[(dvalid, \"valid1\")],\n",
    "        early_stopping_rounds=5\n",
    "    )\n",
    "    \n",
    "    # Saving models and exporting performance metrics\n",
    "    \n",
    "    df_eval_metrics = pd.DataFrame(output[\"history\"][\"valid1\"])\n",
    "    model = output[\"booster\"]\n",
    "    best_model = model[: model.best_iteration]\n",
    "    logging.info(f\"Best model: {best_model}\")\n",
    "    temp_dir = \"/tmp/xgboost\"\n",
    "    os.mkdir(temp_dir)\n",
    "    best_model.save_model(\"{}/{}\".format(temp_dir, args.model_name))\n",
    "    df_eval_metrics.to_json(\"{}/all_results.json\".format(temp_dir))\n",
    "\n",
    "    utils.save_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defad8e8-c925-4e38-8545-20bff43bb3bd",
   "metadata": {},
   "source": [
    "## Wrapping Code (Custom Container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee3403c9-df5c-44dd-b3ad-af71a7ff01de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/Dockerfile\n",
    "\n",
    "FROM rapidsai/rapidsai-nightly:22.04-cuda11.2-base-ubuntu20.04-py3.9\n",
    "\n",
    "RUN pip install google.cloud[storage] \\\n",
    "  && pip install gcsfs \\\n",
    "  && pip install xgboost --upgrade\n",
    "\n",
    "COPY trainer trainer/\n",
    "\n",
    "ENTRYPOINT [\"python\", \"trainer/task.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76450e-37a6-4f19-983b-8b49e0394fdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create: Repositories and Push Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3daff53-b1b1-4912-a584-f1a15fe6207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $TRAINING_REPOSITORY --location $REGION --repository-format docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b2610c13-93bf-4a9a-9f47-90243303cf51",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 7 file(s) totalling 7.4 KiB before compression.\n",
      "Uploading tarball of [training/.] to [gs://jchavezar-demo_cloudbuild/source/1659231973.564417-245115c3229644fb961308575895e0e4.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/359a736d-7d39-4379-9cd8-2d47e0becfd0].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/359a736d-7d39-4379-9cd8-2d47e0becfd0?project=569083142710].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"359a736d-7d39-4379-9cd8-2d47e0becfd0\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1659231973.564417-245115c3229644fb961308575895e0e4.tgz#1659231973772453\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1659231973.564417-245115c3229644fb961308575895e0e4.tgz#1659231973772453...\n",
      "/ [1 files][  2.6 KiB/  2.6 KiB]                                                \n",
      "Operation completed over 1 objects/2.6 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  14.85kB\n",
      "Step 1/4 : FROM rapidsai/rapidsai-nightly:22.04-cuda11.2-base-ubuntu20.04-py3.9\n",
      "22.04-cuda11.2-base-ubuntu20.04-py3.9: Pulling from rapidsai/rapidsai-nightly\n",
      "4d32b49e2995: Pulling fs layer\n",
      "4280f7395d22: Pulling fs layer\n",
      "57b23467550b: Pulling fs layer\n",
      "a3ec9f809e5a: Pulling fs layer\n",
      "94bd43775484: Pulling fs layer\n",
      "deef9dbfbd52: Pulling fs layer\n",
      "5422f080456b: Pulling fs layer\n",
      "77c56b11ad39: Pulling fs layer\n",
      "85b485c64f5c: Pulling fs layer\n",
      "6ba5c77f5862: Pulling fs layer\n",
      "0488f2dc3790: Pulling fs layer\n",
      "a3ec9f809e5a: Waiting\n",
      "94bd43775484: Waiting\n",
      "deef9dbfbd52: Waiting\n",
      "5422f080456b: Waiting\n",
      "77c56b11ad39: Waiting\n",
      "85b485c64f5c: Waiting\n",
      "6ba5c77f5862: Waiting\n",
      "0488f2dc3790: Waiting\n",
      "4280f7395d22: Download complete\n",
      "4d32b49e2995: Verifying Checksum\n",
      "4d32b49e2995: Download complete\n",
      "94bd43775484: Verifying Checksum\n",
      "94bd43775484: Download complete\n",
      "a3ec9f809e5a: Verifying Checksum\n",
      "a3ec9f809e5a: Download complete\n",
      "57b23467550b: Verifying Checksum\n",
      "57b23467550b: Download complete\n",
      "5422f080456b: Verifying Checksum\n",
      "5422f080456b: Download complete\n",
      "77c56b11ad39: Verifying Checksum\n",
      "77c56b11ad39: Download complete\n",
      "4d32b49e2995: Pull complete\n",
      "4280f7395d22: Pull complete\n",
      "57b23467550b: Pull complete\n",
      "a3ec9f809e5a: Pull complete\n",
      "94bd43775484: Pull complete\n",
      "deef9dbfbd52: Verifying Checksum\n",
      "deef9dbfbd52: Download complete\n",
      "0488f2dc3790: Verifying Checksum\n",
      "0488f2dc3790: Download complete\n",
      "85b485c64f5c: Verifying Checksum\n",
      "85b485c64f5c: Download complete\n",
      "6ba5c77f5862: Verifying Checksum\n",
      "6ba5c77f5862: Download complete\n",
      "deef9dbfbd52: Pull complete\n",
      "5422f080456b: Pull complete\n",
      "77c56b11ad39: Pull complete\n",
      "85b485c64f5c: Pull complete\n",
      "6ba5c77f5862: Pull complete\n",
      "0488f2dc3790: Pull complete\n",
      "Digest: sha256:dbdbf166dfb0997d1b138128cf9929df0085e472b8119644044011467b415baf\n",
      "Status: Downloaded newer image for rapidsai/rapidsai-nightly:22.04-cuda11.2-base-ubuntu20.04-py3.9\n",
      " ---> 6f5057ed56a0\n",
      "Step 2/4 : RUN pip install google.cloud[storage]   && pip install gcsfs   && pip install xgboost --upgrade\n",
      " ---> Running in 96a2846bbdf2\n",
      "Collecting google.cloud[storage]\n",
      "  Downloading google_cloud-0.34.0-py2.py3-none-any.whl (1.8 kB)\n",
      "\u001b[91mWARNING: google-cloud 0.34.0 does not provide the extra 'storage'\n",
      "\u001b[0mInstalling collected packages: google.cloud\n",
      "Successfully installed google.cloud-0.34.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mCollecting gcsfs\n",
      "  Downloading gcsfs-2022.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.8/167.8 KB 17.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/rapids/lib/python3.9/site-packages (from gcsfs) (3.8.1)\n",
      "Requirement already satisfied: decorator>4.1.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from gcsfs) (5.1.1)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.4.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 KB 17.2 MB/s eta 0:00:00\n",
      "Collecting fsspec==2022.7.1\n",
      "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.2/141.2 KB 21.5 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.5.2-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/rapids/lib/python3.9/site-packages (from gcsfs) (2.27.1)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 KB 14.8 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from google-auth>=1.2->gcsfs) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from google-auth>=1.2->gcsfs) (5.0.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->gcsfs) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->gcsfs) (2.0.12)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->gcsfs) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->gcsfs) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->gcsfs) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->gcsfs) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->gcsfs) (4.0.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 KB 13.0 MB/s eta 0:00:00\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.6/114.6 KB 18.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->gcsfs) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->gcsfs) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->gcsfs) (1.26.9)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.7/211.7 KB 26.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<5.0.0dev,>=3.15.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (3.19.4)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (36 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 KB 14.0 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.5/151.5 KB 23.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, googleapis-common-protos, google-crc32c, fsspec, requests-oauthlib, google-resumable-media, google-auth, google-auth-oauthlib, google-api-core, google-cloud-core, google-cloud-storage, gcsfs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.3.0\n",
      "    Uninstalling fsspec-2022.3.0:\n",
      "      Successfully uninstalled fsspec-2022.3.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 22.4.0a0+305.gb50ae820bb requires cupy-cuda115, which is not installed.\n",
      "cudf 22.4.0a0+305.gb50ae820bb requires cupy-cuda115, which is not installed.\n",
      "cudf 22.4.0a0+305.gb50ae820bb requires Cython<0.30,>=0.29, which is not installed.\n",
      "cudf-kafka 22.4.0a0+305.gb50ae820bb requires cython, which is not installed.\n",
      "\u001b[0mSuccessfully installed fsspec-2022.7.1 gcsfs-2022.7.1 google-api-core-2.8.2 google-auth-2.9.1 google-auth-oauthlib-0.5.2 google-cloud-core-2.3.2 google-cloud-storage-2.4.0 google-crc32c-1.3.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 oauthlib-3.2.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRequirement already satisfied: xgboost in /opt/conda/envs/rapids/lib/python3.9/site-packages (1.5.2)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 192.9/192.9 MB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/rapids/lib/python3.9/site-packages (from xgboost) (1.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/rapids/lib/python3.9/site-packages (from xgboost) (1.21.5)\n",
      "Installing collected packages: xgboost\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 1.5.2\n",
      "    Uninstalling xgboost-1.5.2:\n",
      "      Successfully uninstalled xgboost-1.5.2\n",
      "Successfully installed xgboost-1.6.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 96a2846bbdf2\n",
      " ---> 1ef4393cc6a6\n",
      "Step 3/4 : COPY trainer trainer/\n",
      " ---> e9a5d32a232b\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"trainer/task.py\"]\n",
      " ---> Running in 6a753da2f25d\n",
      "Removing intermediate container 6a753da2f25d\n",
      " ---> 66a756f2f11f\n",
      "Successfully built 66a756f2f11f\n",
      "Successfully tagged us-central1-docker.pkg.dev/jchavezar-demo/trainings/train_gpu_xgb:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/jchavezar-demo/trainings/train_gpu_xgb:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/jchavezar-demo/trainings/train_gpu_xgb]\n",
      "c04013a75623: Preparing\n",
      "090c7bbe9479: Preparing\n",
      "6a2bd20a3982: Preparing\n",
      "e060177f7dba: Preparing\n",
      "4787e2396578: Preparing\n",
      "2a91749f56f5: Preparing\n",
      "25fea57855d6: Preparing\n",
      "b03eeca41527: Preparing\n",
      "264732e4a10b: Preparing\n",
      "8bacc89280f3: Preparing\n",
      "b5dcd1e3350d: Preparing\n",
      "97d48e8f7e67: Preparing\n",
      "867d0767a47c: Preparing\n",
      "2a91749f56f5: Waiting\n",
      "25fea57855d6: Waiting\n",
      "b03eeca41527: Waiting\n",
      "264732e4a10b: Waiting\n",
      "8bacc89280f3: Waiting\n",
      "b5dcd1e3350d: Waiting\n",
      "97d48e8f7e67: Waiting\n",
      "867d0767a47c: Waiting\n",
      "e060177f7dba: Layer already exists\n",
      "4787e2396578: Layer already exists\n",
      "6a2bd20a3982: Layer already exists\n",
      "2a91749f56f5: Layer already exists\n",
      "25fea57855d6: Layer already exists\n",
      "b03eeca41527: Layer already exists\n",
      "264732e4a10b: Layer already exists\n",
      "8bacc89280f3: Layer already exists\n",
      "b5dcd1e3350d: Layer already exists\n",
      "97d48e8f7e67: Layer already exists\n",
      "867d0767a47c: Layer already exists\n",
      "c04013a75623: Pushed\n",
      "090c7bbe9479: Pushed\n",
      "latest: digest: sha256:9cfd4405a9074a1beba6dabca4cee75dc6c46797d0d235296a78ead79ae1d805 size: 3065\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                                       STATUS\n",
      "359a736d-7d39-4379-9cd8-2d47e0becfd0  2022-07-31T01:46:13+00:00  6M16S     gs://jchavezar-demo_cloudbuild/source/1659231973.564417-245115c3229644fb961308575895e0e4.tgz  us-central1-docker.pkg.dev/jchavezar-demo/trainings/train_gpu_xgb (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit -t $IMAGE_URI training/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128fb06-e3d4-4775-89ee-1a09d5c33dd3",
   "metadata": {},
   "source": [
    "## Create: Vertex Pipe Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1afce7-600e-43f6-90e4-ddd1be60d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (component, Input, Output, Model, Metrics, Model)\n",
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "    ],\n",
    ")\n",
    "def get_train_job_details(\n",
    "    model_dir: str,\n",
    "    model_display_name: str,\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    eval_metric_key: str\n",
    "    ) -> NamedTuple(\n",
    "    \"Outputs\", [(\"eval_metric\", float)]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    metrics_uri = \"{}/model/xgboost/all_results.json\".format(model_dir)\n",
    "    metrics_df = pd.read_json(metrics_uri, typ=\"series\")\n",
    "    for k,v in metrics_df.items():\n",
    "        logging.info(f\"    {k} -> {v}\")\n",
    "        metrics.log_metric(k, min(v.values()))\n",
    "        \n",
    "    eval_metric = (min(metrics_df[eval_metric_key].values()) if eval_metric_key in metrics_df.keys() else None)\n",
    "    model.metadata[eval_metric_key] = eval_metric\n",
    "    outputs = namedtuple(\"Outputs\", [\"eval_metric\"])\n",
    "    \n",
    "    return outputs(eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29adaf9b-4066-44a0-b696-2e9f7439f1bb",
   "metadata": {},
   "source": [
    "## Create: Prediction Custom Container Vertex|Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3087f0-3e36-4e56-b073-60f930cf9161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/app.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from flask import Flask, request, Response, jsonify\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client(project=os.environ['PROJECT_ID'])\n",
    "\n",
    "# Model Download from gcs\n",
    "\n",
    "fname = \"model.json\"\n",
    "\n",
    "with open(fname, \"wb\") as model:\n",
    "    client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/{fname}\", model\n",
    "    )\n",
    "\n",
    "# Loading model\n",
    "print(\"Loading model from: {}\".format(fname))\n",
    "model = xgb.Booster(model_file=fname)\n",
    "\n",
    "# Creation of the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Flask route for Liveness checks\n",
    "@app.route(os.environ['AIP_HEALTH_ROUTE'])\n",
    "def isalive():\n",
    "    status_code = Response(status=200)\n",
    "    return status_code\n",
    "\n",
    "# Flask route for predictions\n",
    "@app.route(os.environ['AIP_PREDICT_ROUTE'],methods=['GET','POST'])\n",
    "def prediction():\n",
    "    _features = ['Id','Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "                          'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm','Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', \n",
    "                          'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9',\n",
    "                          'Soil_Type10','Soil_Type11','Soil_Type12','Soil_Type13','Soil_Type14','Soil_Type15','Soil_Type16','Soil_Type17','Soil_Type18','Soil_Type19', \n",
    "                          'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29',\n",
    "                          'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n",
    "    data = request.get_json(silent=True, force=True)\n",
    "    dmf = xgb.DMatrix(pd.DataFrame(data[\"instances\"], columns=_features))\n",
    "    response = pd.DataFrame(model.predict(dmf))\n",
    "    logging.info(f\"Response: {response}\")\n",
    "    return jsonify({\"Cover Type\": str(response.idxmax(axis=1)[0])})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa60ef-8088-447b-914f-812294877734",
   "metadata": {},
   "source": [
    "### Preparing Docker Container Declarative Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086026cc-fff9-454a-8608-cc413d222701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prediction/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/requirements.txt\n",
    "\n",
    "google-cloud-storage\n",
    "numpy\n",
    "pandas\n",
    "flask\n",
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05bfdf9f-f5f8-4e6e-bb1d-ae4dc335a0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/Dockerfile\n",
    "\n",
    "FROM python:3.7-buster\n",
    "\n",
    "RUN mkdir my-model\n",
    "\n",
    "COPY app.py ./app.py\n",
    "COPY requirements.txt ./requirements.txt\n",
    "RUN pip install -r requirements.txt \n",
    "\n",
    "# Flask Env Variable\n",
    "ENV FLASK_APP=app\n",
    "\n",
    "# Expose port 8080\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD flask run --host=0.0.0.0 --port=8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725956dc-2ee0-43cd-96bb-1f64796ddc44",
   "metadata": {},
   "source": [
    "### Push Container Image to Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37a7f8-8eb6-4066-867f-d2c868cdd94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $PREDICTION_REPOSITORY --location $REGION --repository-format docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "34444603-e3ae-4d4a-85ca-ab8e7017fb15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 5 file(s) totalling 131.4 KiB before compression.\n",
      "Uploading tarball of [prediction/.] to [gs://jchavezar-demo_cloudbuild/source/1659361280.259642-01da885bed7246298da7751489b7144b.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/dc057492-3f62-4528-b3ea-870fa07e3ba8].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/dc057492-3f62-4528-b3ea-870fa07e3ba8?project=569083142710].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"dc057492-3f62-4528-b3ea-870fa07e3ba8\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1659361280.259642-01da885bed7246298da7751489b7144b.tgz#1659361280532657\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1659361280.259642-01da885bed7246298da7751489b7144b.tgz#1659361280532657...\n",
      "/ [1 files][ 34.8 KiB/ 34.8 KiB]                                                \n",
      "Operation completed over 1 objects/34.8 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  140.3kB\n",
      "Step 1/8 : FROM python:3.7-buster\n",
      "3.7-buster: Pulling from library/python\n",
      "80b89a2b88b2: Already exists\n",
      "5b0405f798f5: Pulling fs layer\n",
      "ab80b2b0494a: Pulling fs layer\n",
      "bb827974c1cb: Pulling fs layer\n",
      "be554c4b061a: Pulling fs layer\n",
      "da90315045b8: Pulling fs layer\n",
      "5990d943d736: Pulling fs layer\n",
      "00715acb3b3c: Pulling fs layer\n",
      "c1060b8f9310: Pulling fs layer\n",
      "be554c4b061a: Waiting\n",
      "da90315045b8: Waiting\n",
      "5990d943d736: Waiting\n",
      "00715acb3b3c: Waiting\n",
      "c1060b8f9310: Waiting\n",
      "5b0405f798f5: Verifying Checksum\n",
      "5b0405f798f5: Download complete\n",
      "ab80b2b0494a: Verifying Checksum\n",
      "ab80b2b0494a: Download complete\n",
      "da90315045b8: Verifying Checksum\n",
      "da90315045b8: Download complete\n",
      "5990d943d736: Download complete\n",
      "bb827974c1cb: Verifying Checksum\n",
      "bb827974c1cb: Download complete\n",
      "00715acb3b3c: Verifying Checksum\n",
      "00715acb3b3c: Download complete\n",
      "c1060b8f9310: Verifying Checksum\n",
      "c1060b8f9310: Download complete\n",
      "5b0405f798f5: Pull complete\n",
      "ab80b2b0494a: Pull complete\n",
      "be554c4b061a: Verifying Checksum\n",
      "be554c4b061a: Download complete\n",
      "bb827974c1cb: Pull complete\n",
      "be554c4b061a: Pull complete\n",
      "da90315045b8: Pull complete\n",
      "5990d943d736: Pull complete\n",
      "00715acb3b3c: Pull complete\n",
      "c1060b8f9310: Pull complete\n",
      "Digest: sha256:1317080ef9c8e5e6b9302404ab6217305d28c2c623ac7bcfae540636fef48270\n",
      "Status: Downloaded newer image for python:3.7-buster\n",
      " ---> 2f0e59bbc9aa\n",
      "Step 2/8 : RUN mkdir my-model\n",
      " ---> Running in 160e33cf57e2\n",
      "Removing intermediate container 160e33cf57e2\n",
      " ---> 99d710e0c0e0\n",
      "Step 3/8 : COPY requirements.txt ./requirements.txt\n",
      " ---> 3f8bd066676e\n",
      "Step 4/8 : RUN pip install -r requirements.txt\n",
      " ---> Running in 907a4cc912b3\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.4.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 KB 4.4 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 32.8 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 46.6 MB/s eta 0:00:00\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting simplejson\n",
      "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.3/130.3 KB 18.3 MB/s eta 0:00:00\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.6/76.6 MB 14.0 MB/s eta 0:00:00\n",
      "Collecting flask\n",
      "  Downloading Flask-2.1.3-py3-none-any.whl (95 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.6/95.6 KB 12.2 MB/s eta 0:00:00\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 192.9/192.9 MB 5.3 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 KB 10.3 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.8/167.8 KB 20.5 MB/s eta 0:00:00\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 KB 9.9 MB/s eta 0:00:00\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.6/114.6 KB 16.3 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 503.5/503.5 KB 39.2 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 KB 30.3 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.8/24.8 MB 29.5 MB/s eta 0:00:00\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 KB 7.5 MB/s eta 0:00:00\n",
      "Collecting six\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.1/38.1 MB 22.6 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.2/11.2 MB 52.1 MB/s eta 0:00:00\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.9.0-py2.py3-none-any.whl (15.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 63.4 MB/s eta 0:00:00\n",
      "Collecting Jinja2>=3.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 16.3 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata>=3.6.0\n",
      "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting Werkzeug>=2.0\n",
      "  Downloading Werkzeug-2.2.1-py3-none-any.whl (232 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.4/232.4 KB 27.2 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting click>=8.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 KB 13.3 MB/s eta 0:00:00\n",
      "Collecting protobuf<5.0.0dev,>=3.15.0\n",
      "  Downloading protobuf-4.21.4-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 408.4/408.4 KB 32.9 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.7/211.7 KB 25.3 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 KB 19.1 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting typing-extensions>=3.6.4\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.2/160.2 KB 20.8 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.9/139.9 KB 18.3 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 KB 10.1 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 KB 14.2 MB/s eta 0:00:00\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 82.1 MB/s eta 0:00:00\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 KB 5.7 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.1/944.1 KB 57.3 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 58.6 MB/s eta 0:00:00\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.0/307.0 KB 32.5 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 KB 10.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=cd74c22d3d5ac4e7bec7ac47d3266eb189caf441ac8ad56d5f30b474deebb778\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: pytz, pyasn1, zipp, urllib3, typing-extensions, threadpoolctl, tenacity, six, simplejson, rsa, pyparsing, pyasn1-modules, protobuf, pillow, numpy, MarkupSafe, joblib, itsdangerous, idna, graphviz, google-crc32c, fonttools, cycler, charset-normalizer, certifi, cachetools, Werkzeug, scipy, requests, python-dateutil, plotly, packaging, kiwisolver, Jinja2, importlib-metadata, googleapis-common-protos, google-resumable-media, google-auth, xgboost, scikit-learn, pandas, matplotlib, google-api-core, click, sklearn, google-cloud-core, flask, catboost, google-cloud-storage\n",
      "Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.1 Werkzeug-2.2.1 cachetools-5.2.0 catboost-1.0.6 certifi-2022.6.15 charset-normalizer-2.1.0 click-8.1.3 cycler-0.11.0 flask-2.1.3 fonttools-4.34.4 google-api-core-2.8.2 google-auth-2.9.1 google-cloud-core-2.3.2 google-cloud-storage-2.4.0 google-crc32c-1.3.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 graphviz-0.20.1 idna-3.3 importlib-metadata-4.12.0 itsdangerous-2.1.2 joblib-1.1.0 kiwisolver-1.4.4 matplotlib-3.5.2 numpy-1.21.6 packaging-21.3 pandas-1.3.5 pillow-9.2.0 plotly-5.9.0 protobuf-4.21.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2022.1 requests-2.28.1 rsa-4.9 scikit-learn-1.0.2 scipy-1.7.3 simplejson-3.17.6 six-1.16.0 sklearn-0.0 tenacity-8.0.1 threadpoolctl-3.1.0 typing-extensions-4.3.0 urllib3-1.26.11 xgboost-1.6.1 zipp-3.8.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 907a4cc912b3\n",
      " ---> 80ed0eef7f51\n",
      "Step 5/8 : COPY app.py ./app.py\n",
      " ---> 74a022bbb76e\n",
      "Step 6/8 : ENV FLASK_APP=app\n",
      " ---> Running in f24b5d69785d\n",
      "Removing intermediate container f24b5d69785d\n",
      " ---> 8ff70d99fb23\n",
      "Step 7/8 : EXPOSE 8080\n",
      " ---> Running in bd341637aa61\n",
      "Removing intermediate container bd341637aa61\n",
      " ---> 1f836ab4d45c\n",
      "Step 8/8 : CMD flask run --host=0.0.0.0 --port=8080\n",
      " ---> Running in 4ca49e561ef4\n",
      "Removing intermediate container 4ca49e561ef4\n",
      " ---> 914314a974b6\n",
      "Successfully built 914314a974b6\n",
      "Successfully tagged us-central1-docker.pkg.dev/jchavezar-demo/predictions/prediction:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/jchavezar-demo/predictions/prediction:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/jchavezar-demo/predictions/prediction]\n",
      "13dad0e3f1e2: Preparing\n",
      "3574171ceb4f: Preparing\n",
      "1a001333dc5f: Preparing\n",
      "d8fa5b28c00c: Preparing\n",
      "3067f9dc0b94: Preparing\n",
      "91f66d2a2e47: Preparing\n",
      "6d95eb15aaa8: Preparing\n",
      "f6d434af1a4b: Preparing\n",
      "5339eeb11bed: Preparing\n",
      "4afd7e355699: Preparing\n",
      "92a02f6117f5: Preparing\n",
      "979117655379: Preparing\n",
      "86eecfa8066e: Preparing\n",
      "91f66d2a2e47: Waiting\n",
      "6d95eb15aaa8: Waiting\n",
      "f6d434af1a4b: Waiting\n",
      "5339eeb11bed: Waiting\n",
      "4afd7e355699: Waiting\n",
      "92a02f6117f5: Waiting\n",
      "979117655379: Waiting\n",
      "86eecfa8066e: Waiting\n",
      "3067f9dc0b94: Layer already exists\n",
      "91f66d2a2e47: Layer already exists\n",
      "6d95eb15aaa8: Layer already exists\n",
      "f6d434af1a4b: Layer already exists\n",
      "5339eeb11bed: Layer already exists\n",
      "d8fa5b28c00c: Pushed\n",
      "13dad0e3f1e2: Pushed\n",
      "4afd7e355699: Layer already exists\n",
      "1a001333dc5f: Pushed\n",
      "979117655379: Layer already exists\n",
      "92a02f6117f5: Layer already exists\n",
      "86eecfa8066e: Layer already exists\n",
      "3574171ceb4f: Pushed\n",
      "latest: digest: sha256:2d0fc68a5772d8bcb13c67ec1d4fcf32a04c997907d0d07281a89d8400e4c703 size: 3053\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                                      STATUS\n",
      "dc057492-3f62-4528-b3ea-870fa07e3ba8  2022-08-01T13:41:20+00:00  3M34S     gs://jchavezar-demo_cloudbuild/source/1659361280.259642-01da885bed7246298da7751489b7144b.tgz  us-central1-docker.pkg.dev/jchavezar-demo/predictions/prediction (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $PREDICTION_IMAGE_URI prediction/. --timeout 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f5e54-1647-42e2-b15c-235ae18b163e",
   "metadata": {},
   "source": [
    "## Create: Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afac2a5d-347d-4d73-80a5-12dc9d08d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import pipeline, Condition\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components import aiplatform as gcc\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp as custom_job\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from kfp.v2.components import importer_node\n",
    "\n",
    "\n",
    "@pipeline(name='dask-gpu-1')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    custom_train_name: str,\n",
    "    region: str,\n",
    "    eval_acc_threshold: float,\n",
    "    eval_metric_key: str,\n",
    "    model_file_bucket: str,\n",
    "):\n",
    "    worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"a2-highgpu-1g\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_A100\",\n",
    "            \"accelerator_count\": 1\n",
    "        },\n",
    "        \"replica_count\": \"1\",\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"env\": [{\"name\": \"AIP_TRAINING_DATA_URI\", \"value\":'test'}],\n",
    "            \"command\": [\n",
    "                \"python\",\n",
    "                \"trainer/task.py\"\n",
    "            ],\n",
    "            \"args\": [\n",
    "                \"--dataset_source\", \"gs://vtx-datasets-public/cover_type_4Mrows.csv\",\n",
    "                \"--model-name\", \"model.json\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    ]\n",
    "    train_with_cpu_task = custom_job(\n",
    "        project=project_id,\n",
    "        display_name=custom_train_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_directory=model_file_bucket\n",
    "    )\n",
    "    get_train_details_task = get_train_job_details(\n",
    "        model_dir=MODEL_FILE_BUCKET,\n",
    "        model_display_name=\"xgboost-dask\",\n",
    "        eval_metric_key=eval_metric_key, # mlogloss\n",
    "    ).after(train_with_cpu_task)\n",
    "    \n",
    "    with Condition(\n",
    "        get_train_details_task.outputs[\"eval_metric\"] > eval_acc_threshold,\n",
    "        name=\"model-deploy-decision\",\n",
    "    ):\n",
    "        import_unmanaged_model_op = importer_node.importer(\n",
    "            artifact_uri=\"gs://vtx-models/model/xgboost\",\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": PREDICTION_IMAGE_URI,\n",
    "                    \"env\": [\n",
    "                        {\n",
    "                            \"name\": \"PROJECT_ID\",\n",
    "                            \"value\": PROJECT_ID},\n",
    "                    ],\n",
    "                    \"predictRoute\": \"/predict\",\n",
    "                    \"healthRoute\": \"/health\",\n",
    "                    \"ports\": [\n",
    "                        {\n",
    "                            \"containerPort\": 8080\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        custom_model_upload_job = gcc.ModelUploadOp(\n",
    "            project=project_id,\n",
    "            display_name=\"xgb-model\",\n",
    "            unmanaged_container_model=import_unmanaged_model_op.outputs[\"artifact\"],\n",
    "        ).after(import_unmanaged_model_op)\n",
    "        \n",
    "        endpoint_create_job = gcc.EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            display_name=\"pipelines-created-endpoint\",\n",
    "        )\n",
    "        \n",
    "        custom_model_deploy_job = (gcc.ModelDeployOp(\n",
    "            model=custom_model_upload_job.outputs[\"model\"],\n",
    "            endpoint=endpoint_create_job.outputs[\"endpoint\"],\n",
    "            deployed_model_display_name=\"xgboost_model_end\",\n",
    "            traffic_split={\"0\":\"100\"},\n",
    "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1\n",
    "        )).set_caching_options(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8c3de-97f0-410e-8293-cd3d77bf70f1",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa4335c-36f7-438e-9d46-c22ab5b1e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='dask_cpu.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572bb69-8054-46bd-939b-252ee0b3b7ec",
   "metadata": {},
   "source": [
    "## Run Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c998a74-c573-4732-bc80-6ab24877e38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/dask-gpu-1-20220801130945\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/dask-gpu-1-20220801130945')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/dask-gpu-1-20220801130945?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"dask_cpu\",\n",
    "    template_path=\"dask_cpu.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        'custom_train_name': CUSTOM_TRAIN_NAME,\n",
    "        'region': REGION,\n",
    "        'eval_acc_threshold': 0.5,\n",
    "        'eval_metric_key': 'mlogloss', # mlogloss\n",
    "        'model_file_bucket': MODEL_FILE_BUCKET,\n",
    "    },\n",
    ")\n",
    "\n",
    "job.submit(service_account='vtx-pipe@jchavezar-demo.iam.gserviceaccount.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a13c1-fe16-48f4-85e7-cbeb029f7697",
   "metadata": {},
   "source": [
    "![Pipe](images/vertex-pipe-gpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562658e2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('gcp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "61f821d259e852bb8dda541b337ba40be66c16e8431d3e97d4d2c7f8d54d4461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
