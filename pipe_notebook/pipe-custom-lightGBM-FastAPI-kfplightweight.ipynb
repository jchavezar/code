{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Demostrate AI capabilities using lightGBM framework, Vertex Custom Training Containers, Fast API as webserver (Prediction Custom Containers) in a Vertex Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URI = 'gs://vtx-datasets-public/breast_cancer_data.csv'\n",
    "PIPELINE_ROOT_PATH = 'gs://vtx-root-path'\n",
    "PROJECT_ID= 'jchavezar-demo'\n",
    "MODELS_URI = 'gs://vtx-models/lightgbm'\n",
    "PRED_IMAGE_URI = 'us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_lightgbm_cpu:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (component, Output, Artifact)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"sklearn\"\n",
    "        ]\n",
    ")\n",
    "def get_data(\n",
    "    datasource: str,\n",
    "    dataset_xtrain: Output[Artifact],\n",
    "    dataset_ytrain: Output[Artifact],\n",
    "    dataset_xtest: Output[Artifact],\n",
    "    dataset_ytest: Output[Artifact]\n",
    "    ):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(datasource)\n",
    "    X = df[['mean_radius','mean_texture','mean_perimeter','mean_area','mean_smoothness']]\n",
    "    y = df['diagnosis']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "    X_train.to_csv(dataset_xtrain.path + \".csv\", index=False, encoding='utf-8-sig')\n",
    "    X_test.to_csv(dataset_xtest.path + \".csv\", index=False, encoding='utf-8-sig')\n",
    "    y_train.to_csv(dataset_ytrain.path + \".csv\", index=False, encoding='utf-8-sig')\n",
    "    y_test.to_csv(dataset_ytest.path + \".csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (Input)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"lightgbm\",\n",
    "        \"google-cloud-storage\"]\n",
    ")\n",
    "def train(\n",
    "    project_id: str,\n",
    "    dataset_xtrain: Input[Artifact],\n",
    "    dataset_ytrain: Input[Artifact],\n",
    "    model_uri: str\n",
    "    ):\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import lightgbm as lgb\n",
    "    from google.cloud import storage\n",
    "\n",
    "    X_train = pd.read_csv(dataset_xtrain.path+\".csv\")\n",
    "    y_train = pd.read_csv(dataset_ytrain.path+\".csv\").diagnosis\n",
    "    clf = lgb.LGBMClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    file_name = \"/tmp/model.pkl\"\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(clf, file)\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    _bucket = model_uri.split('/')[2]\n",
    "    _suffix = \"/\".join(model_uri.split('/')[3:]).rstrip(\"/\")\n",
    "    bucket = storage_client.get_bucket(_bucket)\n",
    "    print(bucket)\n",
    "    print(_suffix)\n",
    "    blob = bucket.blob(f'{_suffix}/model.pkl')\n",
    "    blob.upload_from_filename('/tmp/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import ClassificationMetrics, Metrics\n",
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"lightgbm\",\n",
    "        \"sklearn\",\n",
    "        \"google-cloud-storage\"\n",
    "        ]\n",
    ")\n",
    "def evaluate_model(\n",
    "    project_id: str,\n",
    "    dataset_xtest: Input[Artifact],\n",
    "    dataset_ytest: Input[Artifact],\n",
    "    model_uri: str,\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    kpi: Output[Metrics]\n",
    "    ) -> NamedTuple(\n",
    "    \"Outputs\", [(\"eval_metric\", float)]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from google.cloud import storage\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score\n",
    "    from collections import namedtuple\n",
    "\n",
    "    X_test = pd.read_csv(dataset_xtest.path+\".csv\")\n",
    "    y_test = pd.read_csv(dataset_ytest.path+\".csv\")\n",
    "\n",
    "    # Load Model File\n",
    "\n",
    "    file_name = '/tmp/model.pkl'\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    _bucket = model_uri.split('/')[2]\n",
    "    _suffix = \"/\".join(model_uri.split('/')[3:]).rstrip(\"/\")\n",
    "    bucket = storage_client.get_bucket(_bucket)\n",
    "    blob = bucket.blob(f'{_suffix}/model.pkl')\n",
    "    blob.download_to_filename(file_name)\n",
    "\n",
    "    with open(file_name, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy))\n",
    "\n",
    "    y_scores = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(\n",
    "        y_true=y_test.to_numpy(), \n",
    "        y_score=y_scores, \n",
    "        pos_label=True)\n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [\"False\", \"True\"],\n",
    "        confusion_matrix(\n",
    "            y_test, y_pred\n",
    "        ).tolist(),\n",
    "    )\n",
    "\n",
    "    kpi.log_metric(\"accuracy\", float(accuracy))\n",
    "    outputs = namedtuple(\"Outputs\", [\"eval_metric\"])\n",
    "\n",
    "    return outputs(float(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Prediction Server (FastAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr custom_6\n",
    "!mkdir custom_6\n",
    "!mkdir custom_6/app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_6/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_6/app/main.py\n",
    "\n",
    "from google.cloud import storage\n",
    "from fastapi import Request, FastAPI\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "x=os.environ['AIP_STORAGE_URI']\n",
    "print(f'[INFO] ------ {x}', file=sys.stderr)\n",
    "\n",
    "# Loading Model File\n",
    "\n",
    "file_name = 'model.pkl'\n",
    "client = storage.Client(project=os.environ['PROJECT_ID'])\n",
    "with open(file_name, \"wb\") as model:\n",
    "    client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/{file_name}\", model\n",
    "    )\n",
    "with open(file_name, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Webserver methods\n",
    "\n",
    "@app.get('/')\n",
    "def get_root():\n",
    "    return {'message': 'Welcome to Breast Cancer Prediction'}\n",
    "@app.get('/health_check')\n",
    "def health():\n",
    "    return 200\n",
    "if os.environ.get('AIP_PREDICT_ROUTE') is not None:\n",
    "    method = os.environ['AIP_PREDICT_ROUTE']\n",
    "else:\n",
    "    method = '/predict'\n",
    "print(method)\n",
    "@app.post(method)\n",
    "async def predict(request: Request):\n",
    "    print(\"----------------- PREDICTING -----------------\")\n",
    "    body = await request.json()\n",
    "    instances = body[\"instances\"]\n",
    "    outputs = model.predict(instances)\n",
    "    print(f'[INFO] ------ {outputs}, {type(outputs)}', file=sys.stderr)\n",
    "    response = outputs.tolist()\n",
    "    print(\"----------------- OUTPUTS -----------------\")\n",
    "    return {\"predictions\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_6/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_6/Dockerfile\n",
    "\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
    "\n",
    "COPY app /app\n",
    "WORKDIR /app\n",
    "RUN pip install joblib google-cloud-storage lightgbm\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
    "\n",
    "EXPOSE 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 1.4 KiB before compression.\n",
      "Uploading tarball of [custom_6/.] to [gs://jchavezar-demo_cloudbuild/source/1660573188.406273-cd70d93f30544fd5b6d88e5fea92fb17.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/bee7f0d7-11f6-44ce-9681-1f211a1de2dd].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/bee7f0d7-11f6-44ce-9681-1f211a1de2dd?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"bee7f0d7-11f6-44ce-9681-1f211a1de2dd\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1660573188.406273-cd70d93f30544fd5b6d88e5fea92fb17.tgz#1660573189140732\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1660573188.406273-cd70d93f30544fd5b6d88e5fea92fb17.tgz#1660573189140732...\n",
      "/ [1 files][  942.0 B/  942.0 B]                                                \n",
      "Operation completed over 1 objects/942.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  4.608kB\n",
      "Step 1/6 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
      "python3.7: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
      "df5590a8898b: Pulling fs layer\n",
      "705bb4cb554e: Pulling fs layer\n",
      "519df5fceacd: Pulling fs layer\n",
      "ccc287cbeddc: Pulling fs layer\n",
      "e3f8e6af58ed: Pulling fs layer\n",
      "aebed27b2d86: Pulling fs layer\n",
      "3b81dff756ff: Pulling fs layer\n",
      "6e4f59a23b58: Pulling fs layer\n",
      "5cdb61eb416d: Pulling fs layer\n",
      "415755650c07: Pulling fs layer\n",
      "82af7648a68a: Pulling fs layer\n",
      "697e3cdf6fdb: Pulling fs layer\n",
      "8d414b25a011: Pulling fs layer\n",
      "3e3cfa737f56: Pulling fs layer\n",
      "1595db7eb9b1: Pulling fs layer\n",
      "d2d3fcd4b870: Pulling fs layer\n",
      "35f87fd78dc8: Pulling fs layer\n",
      "146e266752fc: Pulling fs layer\n",
      "57df244389c0: Pulling fs layer\n",
      "e8391aa94779: Pulling fs layer\n",
      "ccc287cbeddc: Waiting\n",
      "e3f8e6af58ed: Waiting\n",
      "aebed27b2d86: Waiting\n",
      "3b81dff756ff: Waiting\n",
      "6e4f59a23b58: Waiting\n",
      "5cdb61eb416d: Waiting\n",
      "415755650c07: Waiting\n",
      "82af7648a68a: Waiting\n",
      "697e3cdf6fdb: Waiting\n",
      "8d414b25a011: Waiting\n",
      "3e3cfa737f56: Waiting\n",
      "1595db7eb9b1: Waiting\n",
      "d2d3fcd4b870: Waiting\n",
      "35f87fd78dc8: Waiting\n",
      "146e266752fc: Waiting\n",
      "57df244389c0: Waiting\n",
      "e8391aa94779: Waiting\n",
      "705bb4cb554e: Verifying Checksum\n",
      "705bb4cb554e: Download complete\n",
      "519df5fceacd: Verifying Checksum\n",
      "519df5fceacd: Download complete\n",
      "df5590a8898b: Verifying Checksum\n",
      "df5590a8898b: Download complete\n",
      "aebed27b2d86: Verifying Checksum\n",
      "aebed27b2d86: Download complete\n",
      "ccc287cbeddc: Verifying Checksum\n",
      "ccc287cbeddc: Download complete\n",
      "6e4f59a23b58: Verifying Checksum\n",
      "6e4f59a23b58: Download complete\n",
      "3b81dff756ff: Verifying Checksum\n",
      "3b81dff756ff: Download complete\n",
      "415755650c07: Verifying Checksum\n",
      "415755650c07: Download complete\n",
      "5cdb61eb416d: Verifying Checksum\n",
      "5cdb61eb416d: Download complete\n",
      "697e3cdf6fdb: Verifying Checksum\n",
      "697e3cdf6fdb: Download complete\n",
      "82af7648a68a: Verifying Checksum\n",
      "82af7648a68a: Download complete\n",
      "8d414b25a011: Verifying Checksum\n",
      "8d414b25a011: Download complete\n",
      "3e3cfa737f56: Verifying Checksum\n",
      "3e3cfa737f56: Download complete\n",
      "1595db7eb9b1: Verifying Checksum\n",
      "1595db7eb9b1: Download complete\n",
      "d2d3fcd4b870: Verifying Checksum\n",
      "d2d3fcd4b870: Download complete\n",
      "35f87fd78dc8: Verifying Checksum\n",
      "35f87fd78dc8: Download complete\n",
      "146e266752fc: Verifying Checksum\n",
      "146e266752fc: Download complete\n",
      "e8391aa94779: Verifying Checksum\n",
      "e8391aa94779: Download complete\n",
      "57df244389c0: Verifying Checksum\n",
      "57df244389c0: Download complete\n",
      "e3f8e6af58ed: Download complete\n",
      "df5590a8898b: Pull complete\n",
      "705bb4cb554e: Pull complete\n",
      "519df5fceacd: Pull complete\n",
      "ccc287cbeddc: Pull complete\n",
      "e3f8e6af58ed: Pull complete\n",
      "aebed27b2d86: Pull complete\n",
      "3b81dff756ff: Pull complete\n",
      "6e4f59a23b58: Pull complete\n",
      "5cdb61eb416d: Pull complete\n",
      "415755650c07: Pull complete\n",
      "82af7648a68a: Pull complete\n",
      "697e3cdf6fdb: Pull complete\n",
      "8d414b25a011: Pull complete\n",
      "3e3cfa737f56: Pull complete\n",
      "1595db7eb9b1: Pull complete\n",
      "d2d3fcd4b870: Pull complete\n",
      "35f87fd78dc8: Pull complete\n",
      "146e266752fc: Pull complete\n",
      "57df244389c0: Pull complete\n",
      "e8391aa94779: Pull complete\n",
      "Digest: sha256:f5770422a8875fe3d677e1bda724eeba332b53b0a348a9cdde854ba7136c66be\n",
      "Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
      " ---> bc3531b1f244\n",
      "Step 2/6 : COPY app /app\n",
      " ---> b0b2ecb3c8b3\n",
      "Step 3/6 : WORKDIR /app\n",
      " ---> Running in 588cdd46b57e\n",
      "Removing intermediate container 588cdd46b57e\n",
      " ---> 27201a684304\n",
      "Step 4/6 : RUN pip install joblib google-cloud-storage lightgbm\n",
      " ---> Running in 80f9caa64b9a\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.10.0-py2.py3-none-any.whl (167 kB)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "Collecting scikit-learn!=0.22.0\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/site-packages (from lightgbm) (0.37.0)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Collecting protobuf<5.0.0dev,>=3.15.0\n",
      "  Downloading protobuf-4.21.5-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting six>=1.9.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pyasn1, urllib3, six, rsa, pyasn1-modules, protobuf, idna, charset-normalizer, certifi, cachetools, requests, numpy, googleapis-common-protos, google-auth, threadpoolctl, scipy, joblib, google-crc32c, google-api-core, scikit-learn, google-resumable-media, google-cloud-core, lightgbm, google-cloud-storage\n",
      "Successfully installed cachetools-5.2.0 certifi-2022.6.15 charset-normalizer-2.1.0 google-api-core-2.8.2 google-auth-2.10.0 google-cloud-core-2.3.2 google-cloud-storage-2.5.0 google-crc32c-1.3.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 idna-3.3 joblib-1.1.0 lightgbm-3.3.2 numpy-1.21.6 protobuf-4.21.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.28.1 rsa-4.9 scikit-learn-1.0.2 scipy-1.7.3 six-1.16.0 threadpoolctl-3.1.0 urllib3-1.26.11\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 80f9caa64b9a\n",
      " ---> b3d31cdb3c33\n",
      "Step 5/6 : CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
      " ---> Running in 6366ae70ae98\n",
      "Removing intermediate container 6366ae70ae98\n",
      " ---> 5823502b8f8f\n",
      "Step 6/6 : EXPOSE 8080\n",
      " ---> Running in c894b6c07efa\n",
      "Removing intermediate container c894b6c07efa\n",
      " ---> 5728580217d8\n",
      "Successfully built 5728580217d8\n",
      "Successfully tagged us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_lightgbm_cpu:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_lightgbm_cpu:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_lightgbm_cpu]\n",
      "b0162733a748: Preparing\n",
      "1ed3933ef560: Preparing\n",
      "574be7dd3a19: Preparing\n",
      "4a24608ea27b: Preparing\n",
      "c660563b938b: Preparing\n",
      "e313cca5ff4a: Preparing\n",
      "ad6638039b02: Preparing\n",
      "4c503f7b7cec: Preparing\n",
      "49022fdb47b6: Preparing\n",
      "f4cc5099892d: Preparing\n",
      "471ef6255d6e: Preparing\n",
      "0f8cd2835d79: Preparing\n",
      "0616239ad62a: Preparing\n",
      "4eb436881a0f: Preparing\n",
      "7fe8548565b4: Preparing\n",
      "8364a493536b: Preparing\n",
      "c1792902851c: Preparing\n",
      "c272c95c3fb0: Preparing\n",
      "3054497613e6: Preparing\n",
      "d35dc7f4c79e: Preparing\n",
      "dabfe5b2ea81: Preparing\n",
      "5e6a409f30b6: Preparing\n",
      "e313cca5ff4a: Waiting\n",
      "ad6638039b02: Waiting\n",
      "4c503f7b7cec: Waiting\n",
      "49022fdb47b6: Waiting\n",
      "f4cc5099892d: Waiting\n",
      "471ef6255d6e: Waiting\n",
      "0f8cd2835d79: Waiting\n",
      "0616239ad62a: Waiting\n",
      "4eb436881a0f: Waiting\n",
      "7fe8548565b4: Waiting\n",
      "8364a493536b: Waiting\n",
      "c1792902851c: Waiting\n",
      "c272c95c3fb0: Waiting\n",
      "3054497613e6: Waiting\n",
      "d35dc7f4c79e: Waiting\n",
      "dabfe5b2ea81: Waiting\n",
      "5e6a409f30b6: Waiting\n",
      "574be7dd3a19: Layer already exists\n",
      "4a24608ea27b: Layer already exists\n",
      "c660563b938b: Layer already exists\n",
      "4c503f7b7cec: Layer already exists\n",
      "e313cca5ff4a: Layer already exists\n",
      "ad6638039b02: Layer already exists\n",
      "471ef6255d6e: Layer already exists\n",
      "f4cc5099892d: Layer already exists\n",
      "49022fdb47b6: Layer already exists\n",
      "4eb436881a0f: Layer already exists\n",
      "0f8cd2835d79: Layer already exists\n",
      "0616239ad62a: Layer already exists\n",
      "c1792902851c: Layer already exists\n",
      "1ed3933ef560: Pushed\n",
      "7fe8548565b4: Layer already exists\n",
      "8364a493536b: Layer already exists\n",
      "3054497613e6: Layer already exists\n",
      "c272c95c3fb0: Layer already exists\n",
      "d35dc7f4c79e: Layer already exists\n",
      "dabfe5b2ea81: Layer already exists\n",
      "5e6a409f30b6: Layer already exists\n",
      "b0162733a748: Pushed\n",
      "latest: digest: sha256:9872f7cdcc7a85a641e835d2c60ff894c7abcdf7978ea294495853ace0e9afc3 size: 4925\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                                             STATUS\n",
      "bee7f0d7-11f6-44ce-9681-1f211a1de2dd  2022-08-15T14:19:49+00:00  1M22S     gs://jchavezar-demo_cloudbuild/source/1660573188.406273-cd70d93f30544fd5b6d88e5fea92fb17.tgz  us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_lightgbm_cpu (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit -t $PRED_IMAGE_URI custom_6/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import pipeline, Condition\n",
    "from kfp.v2.components import importer_node\n",
    "from google_cloud_pipeline_components import aiplatform as gcc\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "@pipeline(name='lightgbm-light')\n",
    "def pipeline(\n",
    "    datasource: str,\n",
    "    project_id: str,\n",
    "    model_uri: str,\n",
    "    eval_acc_threshold: float,\n",
    "    ):\n",
    "    get_data_task = get_data(datasource)\n",
    "    train_task = train(\n",
    "        project_id,\n",
    "        get_data_task.outputs[\"dataset_xtrain\"], \n",
    "        get_data_task.outputs[\"dataset_ytrain\"],\n",
    "        model_uri,\n",
    "        )\n",
    "    eval_task = evaluate_model(\n",
    "        project_id,\n",
    "        get_data_task.outputs[\"dataset_xtest\"], \n",
    "        get_data_task.outputs[\"dataset_ytest\"],\n",
    "        model_uri).after(train_task)\n",
    "\n",
    "\n",
    "    with Condition(\n",
    "        eval_task.outputs[\"eval_metric\"] > eval_acc_threshold,\n",
    "        name=\"model-deploy-decision\",\n",
    "    ):\n",
    "        import_unmanaged_model_op = importer_node.importer(\n",
    "            artifact_uri=\"gs://vtx-models/lightgbm\",\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": PRED_IMAGE_URI,\n",
    "                    \"env\": [\n",
    "                        {\n",
    "                            \"name\": \"PROJECT_ID\",\n",
    "                            \"value\": PROJECT_ID},\n",
    "                    ],\n",
    "                    \"predictRoute\": \"/predict\",\n",
    "                    \"healthRoute\": \"/health_check\",\n",
    "                    \"ports\": [\n",
    "                        {\n",
    "                            \"containerPort\": 8080\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        custom_model_upload_job = gcc.ModelUploadOp(\n",
    "            project=PROJECT_ID,\n",
    "            display_name=\"lightgbm-model\",\n",
    "            unmanaged_container_model=import_unmanaged_model_op.outputs[\"artifact\"],\n",
    "            ).after(import_unmanaged_model_op)\n",
    "\n",
    "        endpoint_create_job = gcc.EndpointCreateOp(\n",
    "            project=PROJECT_ID,\n",
    "            display_name=\"pipelines-created-endpoint\",\n",
    "        )\n",
    "            \n",
    "        custom_model_deploy_job = (gcc.ModelDeployOp(\n",
    "            model=custom_model_upload_job.outputs[\"model\"],\n",
    "            endpoint=endpoint_create_job.outputs[\"endpoint\"],\n",
    "            deployed_model_display_name=\"lightgbm_model_end\",\n",
    "            traffic_split={\"0\":\"100\"},\n",
    "            dedicated_resources_machine_type=\"n1-standard-2\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1\n",
    "        )).set_caching_options(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='lightgbm-light.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/lightgbm-light-20220815085002\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/lightgbm-light-20220815085002')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/lightgbm-light-20220815085002?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"lightgbm-light\",\n",
    "    template_path=\"lightgbm-light.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        \"datasource\": DATASET_URI,\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"model_uri\": MODELS_URI,\n",
    "        \"eval_acc_threshold\": 0.5,\n",
    "    },\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.submit(service_account='vtx-pipe@jchavezar-demo.iam.gserviceaccount.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/vertex-pipe-lightgbm-cpu.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('gcp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61f821d259e852bb8dda541b337ba40be66c16e8431d3e97d4d2c7f8d54d4461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
