{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='jchavezar-demo' # Change it\n",
    "REGION='us-central1'\n",
    "IMAGE_URI=f'gcr.io/{PROJECT_ID}/custom_train:v1'\n",
    "PIPELINE_ROOT_PATH='gs://vtx-root-path' # Change it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr custom_train\n",
    "!mkdir custom_train\n",
    "!mkdir custom_train/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_train/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_train/trainer/train.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "dataset_path\n",
    "\n",
    "\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight','Acceleration', 'Model Year', 'Origin']\n",
    "dataset = pd.read_csv(dataset_path, names=column_names, na_values = \"?\", comment='\\t',sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset.tail()\n",
    "\n",
    "BUCKET = 'gs://vtx-models' ## Change this\n",
    "dataset.isna().sum()\n",
    "dataset = dataset.dropna()\n",
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "\n",
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "dataset.tail()\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(\"MPG\")\n",
    "train_stats = train_stats.transpose()\n",
    "\n",
    "train_labels = train_dataset.pop('MPG')\n",
    "test_labels = test_dataset.pop('MPG')\n",
    "\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)\n",
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "model = build_model()\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "early_history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, callbacks=[early_stop])\n",
    "\n",
    "# Export model and save to GCS\n",
    "model.save(BUCKET + '/mpg/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dockerfile, Tag it and Push it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_train/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_train/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-6\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-6\n",
      " ---> a2a8733f4caa\n",
      "Step 2/4 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> ea351d920d2a\n",
      "Step 3/4 : COPY trainer /trainer\n",
      " ---> ba7807bf35a4\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]\n",
      " ---> Running in 0e4f3e642281\n",
      "Removing intermediate container 0e4f3e642281\n",
      " ---> e80dfc0f7803\n",
      "Successfully built e80dfc0f7803\n",
      "Successfully tagged gcr.io/jchavezar-demo/custom_train:v1\n"
     ]
    }
   ],
   "source": [
    "!docker build ./custom_train -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/jchavezar-demo/custom_train]\n",
      "\n",
      "\u001b[1B9bb3b6e8: Preparing \n",
      "\u001b[1B4e054661: Preparing \n",
      "\u001b[1B067465ab: Preparing \n",
      "\u001b[1B01851389: Preparing \n",
      "\u001b[1B2e5f040e: Preparing \n",
      "\u001b[1B59dfa907: Preparing \n",
      "\u001b[1B668df2d8: Preparing \n",
      "\u001b[1B767a76ae: Preparing \n",
      "\u001b[1B559b3e11: Preparing \n",
      "\u001b[1Bc5f28369: Preparing \n",
      "\u001b[1Beeca4cbf: Preparing \n",
      "\u001b[1Bc2b66f65: Preparing \n",
      "\u001b[1B0bba959a: Preparing \n",
      "\u001b[1B677fbd36: Preparing \n",
      "\u001b[1B713472f0: Preparing \n",
      "\u001b[1B33654a88: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B5cfc6aa2: Preparing \n",
      "\u001b[3Bbf18a086: Preparing \n",
      "\u001b[1B4b178955: Preparing \n",
      "\u001b[21Bbb3b6e8: Pushed lready exists 6kB\u001b[19A\u001b[2K\u001b[17A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[21A\u001b[2Kv1: digest: sha256:8b4848d9041346a05517d9cdd6bcbff9322eaf8e4a9b7277e4de968dcdfa06f6 size: 4918\n"
     ]
    }
   ],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Worker Specs\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\"\n",
    "        },\n",
    "        \"replica_count\": \"1\",\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import pipeline\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "\n",
    "@pipeline(name='custom-train')\n",
    "def pipeline(\n",
    "    project_id: str\n",
    "):\n",
    "    train_job = CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        display_name='custom_train',\n",
    "        worker_pool_specs=worker_pool_specs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='custom_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/custom-train-20220904011352\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/custom-train-20220904011352')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-train-20220904011352?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"custom_train\",\n",
    "    template_path=\"custom_train.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID\n",
    "    },\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('gcp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61f821d259e852bb8dda541b337ba40be66c16e8431d3e97d4d2c7f8d54d4461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
